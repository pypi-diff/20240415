# Comparing `tmp/pytorch_ood-0.1.7-py3-none-any.whl.zip` & `tmp/pytorch_ood-0.1.8-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,86 +1,89 @@
-Zip file size: 120139 bytes, number of entries: 84
--rw-r--r--  2.0 unx      209 b- defN 24-Jan-31 11:29 pytorch_ood/__init__.py
+Zip file size: 126114 bytes, number of entries: 87
+-rw-r--r--  2.0 unx      209 b- defN 24-Apr-15 11:09 pytorch_ood/__init__.py
 -rw-r--r--  2.0 unx     2341 b- defN 23-Jul-18 10:04 pytorch_ood/api.py
--rw-r--r--  2.0 unx     1404 b- defN 24-Jan-31 11:29 pytorch_ood/benchmark/__init__.py
--rw-r--r--  2.0 unx      876 b- defN 23-Jul-18 10:04 pytorch_ood/benchmark/base.py
--rw-r--r--  2.0 unx      144 b- defN 24-Jan-31 11:29 pytorch_ood/benchmark/img/__init__.py
--rw-r--r--  2.0 unx     7122 b- defN 24-Jan-31 11:29 pytorch_ood/benchmark/img/cifar10.py
--rw-r--r--  2.0 unx     7157 b- defN 24-Jan-31 11:29 pytorch_ood/benchmark/img/cifar100.py
--rw-r--r--  2.0 unx     3922 b- defN 24-Jan-31 11:29 pytorch_ood/benchmark/img/imagenet.py
+-rw-r--r--  2.0 unx     1404 b- defN 24-Apr-15 10:36 pytorch_ood/benchmark/__init__.py
+-rw-r--r--  2.0 unx      859 b- defN 24-Apr-15 10:36 pytorch_ood/benchmark/base.py
+-rw-r--r--  2.0 unx      144 b- defN 24-Apr-04 13:02 pytorch_ood/benchmark/img/__init__.py
+-rw-r--r--  2.0 unx     7639 b- defN 24-Apr-15 10:36 pytorch_ood/benchmark/img/cifar10.py
+-rw-r--r--  2.0 unx     7729 b- defN 24-Apr-15 10:36 pytorch_ood/benchmark/img/cifar100.py
+-rw-r--r--  2.0 unx     4316 b- defN 24-Apr-15 10:36 pytorch_ood/benchmark/img/imagenet.py
 -rw-r--r--  2.0 unx       11 b- defN 23-Jul-18 10:04 pytorch_ood/dataset/__init__.py
 -rw-r--r--  2.0 unx      165 b- defN 23-Jul-18 10:04 pytorch_ood/dataset/audio/__init__.py
--rw-r--r--  2.0 unx     3059 b- defN 23-Jul-18 10:04 pytorch_ood/dataset/audio/fsdd.py
--rw-r--r--  2.0 unx     3596 b- defN 24-Jan-31 11:29 pytorch_ood/dataset/img/__init__.py
--rw-r--r--  2.0 unx     2519 b- defN 24-Jan-31 11:29 pytorch_ood/dataset/img/base.py
--rw-r--r--  2.0 unx     4329 b- defN 23-Jul-18 10:04 pytorch_ood/dataset/img/chars74k.py
--rw-r--r--  2.0 unx     3097 b- defN 24-Jan-31 11:29 pytorch_ood/dataset/img/cifar.py
--rw-r--r--  2.0 unx     1353 b- defN 24-Jan-31 11:29 pytorch_ood/dataset/img/fooling.py
--rw-r--r--  2.0 unx     5793 b- defN 23-Jul-18 10:04 pytorch_ood/dataset/img/imagenet.py
--rw-r--r--  2.0 unx     4270 b- defN 23-Jul-18 10:04 pytorch_ood/dataset/img/mnistc.py
--rw-r--r--  2.0 unx     5382 b- defN 23-Jul-18 10:04 pytorch_ood/dataset/img/mvtech.py
--rw-r--r--  2.0 unx     1919 b- defN 24-Jan-31 11:29 pytorch_ood/dataset/img/ninco.py
--rw-r--r--  2.0 unx     3936 b- defN 23-Jul-18 10:04 pytorch_ood/dataset/img/noise.py
--rw-r--r--  2.0 unx     4891 b- defN 23-Jul-18 10:04 pytorch_ood/dataset/img/odin.py
--rw-r--r--  2.0 unx     4715 b- defN 24-Jan-31 11:29 pytorch_ood/dataset/img/openood.py
--rw-r--r--  2.0 unx     8610 b- defN 24-Jan-31 11:29 pytorch_ood/dataset/img/pixmix.py
--rw-r--r--  2.0 unx     4242 b- defN 23-Jul-18 10:04 pytorch_ood/dataset/img/streethazards.py
--rw-r--r--  2.0 unx     3001 b- defN 23-Jul-18 10:04 pytorch_ood/dataset/img/textures.py
--rw-r--r--  2.0 unx     3780 b- defN 24-Jan-31 11:29 pytorch_ood/dataset/img/tinyimagenet.py
+-rw-r--r--  2.0 unx     3059 b- defN 24-Apr-15 10:36 pytorch_ood/dataset/audio/fsdd.py
+-rw-r--r--  2.0 unx     3754 b- defN 24-Apr-15 10:36 pytorch_ood/dataset/img/__init__.py
+-rw-r--r--  2.0 unx     2519 b- defN 24-Apr-15 10:36 pytorch_ood/dataset/img/base.py
+-rw-r--r--  2.0 unx     4329 b- defN 24-Apr-15 10:36 pytorch_ood/dataset/img/chars74k.py
+-rw-r--r--  2.0 unx     3097 b- defN 24-Apr-15 10:36 pytorch_ood/dataset/img/cifar.py
+-rw-r--r--  2.0 unx     1353 b- defN 24-Apr-04 13:02 pytorch_ood/dataset/img/fooling.py
+-rw-r--r--  2.0 unx     5793 b- defN 24-Apr-15 10:36 pytorch_ood/dataset/img/imagenet.py
+-rw-r--r--  2.0 unx     4270 b- defN 24-Apr-15 10:36 pytorch_ood/dataset/img/mnistc.py
+-rw-r--r--  2.0 unx     5382 b- defN 24-Apr-15 10:36 pytorch_ood/dataset/img/mvtech.py
+-rw-r--r--  2.0 unx     1974 b- defN 24-Apr-15 10:36 pytorch_ood/dataset/img/ninco.py
+-rw-r--r--  2.0 unx     3936 b- defN 24-Apr-15 10:36 pytorch_ood/dataset/img/noise.py
+-rw-r--r--  2.0 unx     4891 b- defN 24-Apr-15 10:36 pytorch_ood/dataset/img/odin.py
+-rw-r--r--  2.0 unx     4639 b- defN 24-Apr-15 10:36 pytorch_ood/dataset/img/openood.py
+-rw-r--r--  2.0 unx     8613 b- defN 24-Apr-15 10:37 pytorch_ood/dataset/img/pixmix.py
+-rw-r--r--  2.0 unx     4242 b- defN 24-Apr-15 10:36 pytorch_ood/dataset/img/streethazards.py
+-rw-r--r--  2.0 unx     4699 b- defN 24-Apr-15 10:35 pytorch_ood/dataset/img/sumnist.py
+-rw-r--r--  2.0 unx     3001 b- defN 24-Apr-15 10:36 pytorch_ood/dataset/img/textures.py
+-rw-r--r--  2.0 unx     3833 b- defN 24-Apr-15 10:36 pytorch_ood/dataset/img/tinyimagenet.py
 -rw-r--r--  2.0 unx     4692 b- defN 23-Jul-18 10:04 pytorch_ood/dataset/img/tinyimages.py
 -rw-r--r--  2.0 unx      507 b- defN 23-Jul-18 10:04 pytorch_ood/dataset/ossim/__init__.py
--rw-r--r--  2.0 unx     8194 b- defN 23-Jul-20 16:02 pytorch_ood/dataset/ossim/ossim.py
+-rw-r--r--  2.0 unx     8194 b- defN 24-Apr-15 10:36 pytorch_ood/dataset/ossim/ossim.py
 -rw-r--r--  2.0 unx     1049 b- defN 23-Jul-18 10:04 pytorch_ood/dataset/txt/__init__.py
--rw-r--r--  2.0 unx     2665 b- defN 24-Jan-31 11:29 pytorch_ood/dataset/txt/multi30k.py
--rw-r--r--  2.0 unx     4002 b- defN 23-Jul-18 10:04 pytorch_ood/dataset/txt/newsgroups.py
--rw-r--r--  2.0 unx     5954 b- defN 23-Jul-18 10:04 pytorch_ood/dataset/txt/reuters.py
+-rw-r--r--  2.0 unx     2665 b- defN 24-Apr-15 10:36 pytorch_ood/dataset/txt/multi30k.py
+-rw-r--r--  2.0 unx     4002 b- defN 24-Apr-15 10:36 pytorch_ood/dataset/txt/newsgroups.py
+-rw-r--r--  2.0 unx     5954 b- defN 24-Apr-15 10:36 pytorch_ood/dataset/txt/reuters.py
 -rw-r--r--  2.0 unx     1983 b- defN 23-Jul-18 10:04 pytorch_ood/dataset/txt/stop_words.py
--rw-r--r--  2.0 unx     3331 b- defN 23-Jul-18 10:04 pytorch_ood/dataset/txt/wiki.py
+-rw-r--r--  2.0 unx     3331 b- defN 24-Apr-15 10:36 pytorch_ood/dataset/txt/wiki.py
 -rw-r--r--  2.0 unx     2034 b- defN 23-Jul-18 10:04 pytorch_ood/dataset/txt/wmt16.py
--rw-r--r--  2.0 unx     3460 b- defN 24-Jan-31 11:29 pytorch_ood/detector/__init__.py
--rw-r--r--  2.0 unx     4750 b- defN 24-Jan-31 11:29 pytorch_ood/detector/ash.py
--rw-r--r--  2.0 unx     3050 b- defN 24-Jan-31 11:29 pytorch_ood/detector/dice.py
--rw-r--r--  2.0 unx     2391 b- defN 24-Jan-31 11:29 pytorch_ood/detector/energy.py
--rw-r--r--  2.0 unx     2085 b- defN 24-Jan-31 11:29 pytorch_ood/detector/entropy.py
--rw-r--r--  2.0 unx     4402 b- defN 23-Jul-18 10:04 pytorch_ood/detector/klmatching.py
--rw-r--r--  2.0 unx     3207 b- defN 24-Jan-31 11:29 pytorch_ood/detector/knn.py
--rw-r--r--  2.0 unx     7841 b- defN 24-Jan-31 11:29 pytorch_ood/detector/mahalanobis.py
--rw-r--r--  2.0 unx     1849 b- defN 24-Jan-31 11:29 pytorch_ood/detector/maxlogit.py
--rw-r--r--  2.0 unx     6286 b- defN 24-Jan-31 11:29 pytorch_ood/detector/mcd.py
--rw-r--r--  2.0 unx     5856 b- defN 24-Jan-31 11:29 pytorch_ood/detector/odin.py
--rw-r--r--  2.0 unx     2691 b- defN 24-Jan-31 11:29 pytorch_ood/detector/react.py
--rw-r--r--  2.0 unx     4787 b- defN 24-Jan-31 11:29 pytorch_ood/detector/rmd.py
--rw-r--r--  2.0 unx     3094 b- defN 24-Jan-31 11:29 pytorch_ood/detector/she.py
--rw-r--r--  2.0 unx     2660 b- defN 24-Jan-31 11:29 pytorch_ood/detector/softmax.py
--rw-r--r--  2.0 unx     3480 b- defN 24-Jan-31 11:29 pytorch_ood/detector/tscaling.py
--rw-r--r--  2.0 unx     5267 b- defN 23-Jul-18 10:04 pytorch_ood/detector/vim.py
+-rw-r--r--  2.0 unx     3597 b- defN 24-Apr-15 08:52 pytorch_ood/detector/__init__.py
+-rw-r--r--  2.0 unx     4774 b- defN 24-Apr-15 08:52 pytorch_ood/detector/ash.py
+-rw-r--r--  2.0 unx     3075 b- defN 24-Apr-15 10:37 pytorch_ood/detector/dice.py
+-rw-r--r--  2.0 unx     2391 b- defN 24-Apr-04 13:02 pytorch_ood/detector/energy.py
+-rw-r--r--  2.0 unx     2085 b- defN 24-Apr-04 13:02 pytorch_ood/detector/entropy.py
+-rw-r--r--  2.0 unx     4402 b- defN 24-Apr-15 10:36 pytorch_ood/detector/klmatching.py
+-rw-r--r--  2.0 unx     3228 b- defN 24-Apr-15 10:38 pytorch_ood/detector/knn.py
+-rw-r--r--  2.0 unx     7823 b- defN 24-Apr-15 10:36 pytorch_ood/detector/mahalanobis.py
+-rw-r--r--  2.0 unx     1849 b- defN 24-Apr-04 13:02 pytorch_ood/detector/maxlogit.py
+-rw-r--r--  2.0 unx     6383 b- defN 24-Apr-15 10:36 pytorch_ood/detector/mcd.py
+-rw-r--r--  2.0 unx     5856 b- defN 24-Apr-15 10:36 pytorch_ood/detector/odin.py
+-rw-r--r--  2.0 unx     2709 b- defN 24-Apr-15 08:52 pytorch_ood/detector/react.py
+-rw-r--r--  2.0 unx     4812 b- defN 24-Apr-15 10:36 pytorch_ood/detector/rmd.py
+-rw-r--r--  2.0 unx     3095 b- defN 24-Apr-15 10:36 pytorch_ood/detector/she.py
+-rw-r--r--  2.0 unx     2660 b- defN 24-Apr-15 10:36 pytorch_ood/detector/softmax.py
+-rw-r--r--  2.0 unx     3480 b- defN 24-Apr-15 10:38 pytorch_ood/detector/tscaling.py
+-rw-r--r--  2.0 unx     5267 b- defN 24-Apr-15 10:36 pytorch_ood/detector/vim.py
+-rw-r--r--  2.0 unx     3436 b- defN 24-Apr-15 10:36 pytorch_ood/detector/webo.py
 -rw-r--r--  2.0 unx      323 b- defN 23-Jul-18 10:04 pytorch_ood/detector/openmax/__init__.py
--rw-r--r--  2.0 unx     1594 b- defN 24-Jan-31 11:29 pytorch_ood/detector/openmax/libnotmr.py
--rw-r--r--  2.0 unx     6787 b- defN 24-Jan-31 11:29 pytorch_ood/detector/openmax/numpy.py
--rw-r--r--  2.0 unx     3317 b- defN 24-Jan-31 11:29 pytorch_ood/detector/openmax/torch.py
--rw-r--r--  2.0 unx     5408 b- defN 23-Jul-18 10:04 pytorch_ood/loss/__init__.py
+-rw-r--r--  2.0 unx     1636 b- defN 24-Apr-15 10:36 pytorch_ood/detector/openmax/libnotmr.py
+-rw-r--r--  2.0 unx     6789 b- defN 24-Apr-15 10:36 pytorch_ood/detector/openmax/numpy.py
+-rw-r--r--  2.0 unx     3317 b- defN 24-Apr-15 10:36 pytorch_ood/detector/openmax/torch.py
+-rw-r--r--  2.0 unx     5819 b- defN 24-Apr-15 10:31 pytorch_ood/loss/__init__.py
 -rw-r--r--  2.0 unx     1589 b- defN 23-Jul-18 10:04 pytorch_ood/loss/background.py
--rw-r--r--  2.0 unx     3877 b- defN 23-Jul-18 10:04 pytorch_ood/loss/cac.py
--rw-r--r--  2.0 unx     4017 b- defN 24-Jan-31 11:29 pytorch_ood/loss/center.py
--rw-r--r--  2.0 unx     2378 b- defN 23-Jul-18 10:04 pytorch_ood/loss/conf.py
+-rw-r--r--  2.0 unx     3877 b- defN 24-Apr-15 10:36 pytorch_ood/loss/cac.py
+-rw-r--r--  2.0 unx     4017 b- defN 24-Apr-04 13:02 pytorch_ood/loss/center.py
+-rw-r--r--  2.0 unx     2378 b- defN 24-Apr-15 10:36 pytorch_ood/loss/conf.py
 -rw-r--r--  2.0 unx     1188 b- defN 23-Jul-18 10:04 pytorch_ood/loss/crossentropy.py
--rw-r--r--  2.0 unx     2565 b- defN 23-Jul-18 10:04 pytorch_ood/loss/energy.py
--rw-r--r--  2.0 unx     2741 b- defN 23-Jul-18 10:04 pytorch_ood/loss/entropy.py
--rw-r--r--  2.0 unx     4599 b- defN 23-Jul-18 10:04 pytorch_ood/loss/ii.py
--rw-r--r--  2.0 unx     4875 b- defN 23-Jul-18 10:04 pytorch_ood/loss/mchad.py
--rw-r--r--  2.0 unx     2667 b- defN 23-Jul-18 10:04 pytorch_ood/loss/objectosphere.py
--rw-r--r--  2.0 unx     2199 b- defN 23-Jul-18 10:04 pytorch_ood/loss/oe.py
--rw-r--r--  2.0 unx     5289 b- defN 23-Jul-18 10:04 pytorch_ood/loss/svdd.py
+-rw-r--r--  2.0 unx     3394 b- defN 24-Apr-15 10:36 pytorch_ood/loss/energy.py
+-rw-r--r--  2.0 unx     2741 b- defN 24-Apr-15 10:36 pytorch_ood/loss/entropy.py
+-rw-r--r--  2.0 unx     4599 b- defN 24-Apr-15 10:36 pytorch_ood/loss/ii.py
+-rw-r--r--  2.0 unx     4875 b- defN 24-Apr-15 10:36 pytorch_ood/loss/mchad.py
+-rw-r--r--  2.0 unx     2667 b- defN 24-Apr-15 10:36 pytorch_ood/loss/objectosphere.py
+-rw-r--r--  2.0 unx     3009 b- defN 24-Apr-15 10:36 pytorch_ood/loss/oe.py
+-rw-r--r--  2.0 unx     5314 b- defN 24-Apr-15 10:35 pytorch_ood/loss/svdd.py
+-rw-r--r--  2.0 unx     4311 b- defN 24-Apr-15 10:58 pytorch_ood/loss/vos.py
 -rw-r--r--  2.0 unx      723 b- defN 23-Jul-18 10:04 pytorch_ood/model/__init__.py
--rw-r--r--  2.0 unx     4684 b- defN 23-Jul-18 10:04 pytorch_ood/model/centers.py
+-rw-r--r--  2.0 unx     4684 b- defN 24-Apr-15 10:36 pytorch_ood/model/centers.py
 -rw-r--r--  2.0 unx     1611 b- defN 23-Jul-18 10:04 pytorch_ood/model/gru.py
--rw-r--r--  2.0 unx    12629 b- defN 24-Jan-31 11:29 pytorch_ood/model/wrn.py
+-rw-r--r--  2.0 unx    12629 b- defN 24-Apr-15 10:36 pytorch_ood/model/wrn.py
 -rw-r--r--  2.0 unx       70 b- defN 23-Jul-18 10:04 pytorch_ood/utils/__init__.py
--rw-r--r--  2.0 unx     7226 b- defN 24-Jan-31 11:29 pytorch_ood/utils/metrics.py
--rw-r--r--  2.0 unx     1953 b- defN 24-Jan-31 11:29 pytorch_ood/utils/transforms.py
--rw-r--r--  2.0 unx     8878 b- defN 23-Jul-18 10:04 pytorch_ood/utils/utils.py
--rw-r--r--  2.0 unx    11351 b- defN 24-Jan-31 11:37 pytorch_ood-0.1.7.dist-info/LICENSE
--rw-r--r--  2.0 unx    23161 b- defN 24-Jan-31 11:37 pytorch_ood-0.1.7.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Jan-31 11:37 pytorch_ood-0.1.7.dist-info/WHEEL
--rw-r--r--  2.0 unx       12 b- defN 24-Jan-31 11:37 pytorch_ood-0.1.7.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     7364 b- defN 24-Jan-31 11:37 pytorch_ood-0.1.7.dist-info/RECORD
-84 files, 328577 bytes uncompressed, 108449 bytes compressed:  67.0%
+-rw-r--r--  2.0 unx     7600 b- defN 24-Apr-15 10:36 pytorch_ood/utils/metrics.py
+-rw-r--r--  2.0 unx     1953 b- defN 24-Apr-04 13:02 pytorch_ood/utils/transforms.py
+-rw-r--r--  2.0 unx     8878 b- defN 24-Apr-15 10:36 pytorch_ood/utils/utils.py
+-rw-r--r--  2.0 unx    11351 b- defN 24-Apr-15 11:16 pytorch_ood-0.1.8.dist-info/LICENSE
+-rw-r--r--  2.0 unx    23385 b- defN 24-Apr-15 11:16 pytorch_ood-0.1.8.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-Apr-15 11:16 pytorch_ood-0.1.8.dist-info/WHEEL
+-rw-r--r--  2.0 unx       12 b- defN 24-Apr-15 11:16 pytorch_ood-0.1.8.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     7620 b- defN 24-Apr-15 11:16 pytorch_ood-0.1.8.dist-info/RECORD
+87 files, 345985 bytes uncompressed, 114026 bytes compressed:  67.0%
```

## zipnote {}

```diff
@@ -69,14 +69,17 @@
 
 Filename: pytorch_ood/dataset/img/pixmix.py
 Comment: 
 
 Filename: pytorch_ood/dataset/img/streethazards.py
 Comment: 
 
+Filename: pytorch_ood/dataset/img/sumnist.py
+Comment: 
+
 Filename: pytorch_ood/dataset/img/textures.py
 Comment: 
 
 Filename: pytorch_ood/dataset/img/tinyimagenet.py
 Comment: 
 
 Filename: pytorch_ood/dataset/img/tinyimages.py
@@ -156,14 +159,17 @@
 
 Filename: pytorch_ood/detector/tscaling.py
 Comment: 
 
 Filename: pytorch_ood/detector/vim.py
 Comment: 
 
+Filename: pytorch_ood/detector/webo.py
+Comment: 
+
 Filename: pytorch_ood/detector/openmax/__init__.py
 Comment: 
 
 Filename: pytorch_ood/detector/openmax/libnotmr.py
 Comment: 
 
 Filename: pytorch_ood/detector/openmax/numpy.py
@@ -207,14 +213,17 @@
 
 Filename: pytorch_ood/loss/oe.py
 Comment: 
 
 Filename: pytorch_ood/loss/svdd.py
 Comment: 
 
+Filename: pytorch_ood/loss/vos.py
+Comment: 
+
 Filename: pytorch_ood/model/__init__.py
 Comment: 
 
 Filename: pytorch_ood/model/centers.py
 Comment: 
 
 Filename: pytorch_ood/model/gru.py
@@ -231,23 +240,23 @@
 
 Filename: pytorch_ood/utils/transforms.py
 Comment: 
 
 Filename: pytorch_ood/utils/utils.py
 Comment: 
 
-Filename: pytorch_ood-0.1.7.dist-info/LICENSE
+Filename: pytorch_ood-0.1.8.dist-info/LICENSE
 Comment: 
 
-Filename: pytorch_ood-0.1.7.dist-info/METADATA
+Filename: pytorch_ood-0.1.8.dist-info/METADATA
 Comment: 
 
-Filename: pytorch_ood-0.1.7.dist-info/WHEEL
+Filename: pytorch_ood-0.1.8.dist-info/WHEEL
 Comment: 
 
-Filename: pytorch_ood-0.1.7.dist-info/top_level.txt
+Filename: pytorch_ood-0.1.8.dist-info/top_level.txt
 Comment: 
 
-Filename: pytorch_ood-0.1.7.dist-info/RECORD
+Filename: pytorch_ood-0.1.8.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## pytorch_ood/__init__.py

```diff
@@ -1,8 +1,8 @@
 """
 PyTorch Out-of-Distribution Detection
 """
-__version__ = "0.1.7"
+__version__ = "0.1.8"
 
 from . import api, dataset, detector, loss, model, utils
 
 __all__ = ["dataset", "detector", "loss", "model", "utils", "api", "__version__"]
```

## pytorch_ood/benchmark/base.py

```diff
@@ -1,11 +1,12 @@
 from abc import ABC, abstractmethod
-from typing import List, Dict
+from typing import Dict, List
 
 from torch.utils.data import Dataset
+
 from pytorch_ood.api import Detector
 
 
 class Benchmark(ABC):
     """
     Base class for Benchmarks
     """
@@ -24,14 +25,12 @@
 
         :param known: include IN
         :param unknown: include OOD
         """
         pass
 
     @abstractmethod
-    def evaluate(
-            self, detector: Detector, *args, **kwargs
-    ) -> List[Dict]:
+    def evaluate(self, detector: Detector, *args, **kwargs) -> List[Dict]:
         """
         Evaluates the given detector on all datasets and returns a list with the results
         """
         pass
```

## pytorch_ood/benchmark/img/cifar10.py

```diff
@@ -4,18 +4,27 @@
 from typing import Dict, List
 
 from torch.utils.data import DataLoader, Dataset
 from torchvision.datasets import CIFAR10, CIFAR100, MNIST, FashionMNIST
 from torchvision.transforms import Compose
 
 from pytorch_ood.api import Detector
-from pytorch_ood.dataset.img import LSUNCrop, LSUNResize, TinyImageNetCrop, TinyImageNetResize, GaussianNoise, \
-    UniformNoise, TinyImageNet, Textures, Places365
-from pytorch_ood.utils import OODMetrics, ToUnknown, ToRGB
 from pytorch_ood.benchmark import Benchmark
+from pytorch_ood.dataset.img import (
+    GaussianNoise,
+    LSUNCrop,
+    LSUNResize,
+    Places365,
+    Textures,
+    TinyImageNet,
+    TinyImageNetCrop,
+    TinyImageNetResize,
+    UniformNoise,
+)
+from pytorch_ood.utils import OODMetrics, ToRGB, ToUnknown
 
 
 class CIFAR10_ODIN(Benchmark):
     """
     Replicates the OOD detection benchmark from the ODIN paper for CIFAR 10.
 
     :see Paper: `ArXiv <https://arxiv.org/abs/1706.02690>`__
@@ -42,22 +51,30 @@
         self.test_oods = [
             TinyImageNetCrop(
                 root, download=True, transform=transform, target_transform=ToUnknown()
             ),
             TinyImageNetResize(
                 root, download=True, transform=transform, target_transform=ToUnknown()
             ),
-            LSUNResize(
-                root, download=True, transform=transform, target_transform=ToUnknown()
-            ),
-            LSUNCrop(
-                root, download=True, transform=transform, target_transform=ToUnknown()
+            LSUNResize(root, download=True, transform=transform, target_transform=ToUnknown()),
+            LSUNCrop(root, download=True, transform=transform, target_transform=ToUnknown()),
+            UniformNoise(
+                1000,
+                size=(32, 32, 3),
+                transform=transform,
+                target_transform=ToUnknown(),
+                seed=123,
+            ),
+            GaussianNoise(
+                1000,
+                size=(32, 32, 3),
+                transform=transform,
+                target_transform=ToUnknown(),
+                seed=123,
             ),
-            UniformNoise(1000, size=(32, 32, 3), transform=transform, target_transform=ToUnknown(), seed=123),
-            GaussianNoise(1000, size=(32, 32, 3), transform=transform, target_transform=ToUnknown(), seed=123)
         ]
 
         self.ood_names: List[str] = []  #: OOD Dataset names
         self.ood_names = [type(d).__name__ for d in self.test_oods]
 
     def train_set(self) -> Dataset:
         """
@@ -82,15 +99,15 @@
 
         if not known and unknown:
             return self.test_oods
 
         raise ValueError()
 
     def evaluate(
-            self, detector: Detector, loader_kwargs: Dict = None, device: str = "cpu"
+        self, detector: Detector, loader_kwargs: Dict = None, device: str = "cpu"
     ) -> List[Dict]:
         """
         Evaluates the given detector on all datasets and returns a list with the results
 
         :param detector: the detector to evaluate
         :param loader_kwargs: keyword arguments to give to the data loader
         :param device: the device to move batches to
@@ -143,31 +160,53 @@
         """
         self.transform = Compose([ToRGB(), transform])
         self.train_in = CIFAR10(root, download=True, transform=transform, train=True)
         self.test_in = CIFAR10(root, download=True, transform=transform, train=False)
 
         self.test_oods = [
             CIFAR100(
-                root, download=True, transform=self.transform, target_transform=ToUnknown(), train=False
+                root,
+                download=True,
+                transform=self.transform,
+                target_transform=ToUnknown(),
+                train=False,
             ),
             TinyImageNet(
-                root, download=True, transform=self.transform, target_transform=ToUnknown(), subset="val"
+                root,
+                download=True,
+                transform=self.transform,
+                target_transform=ToUnknown(),
+                subset="val",
             ),
             MNIST(
-                root, download=True, transform=self.transform, target_transform=ToUnknown(), train=False
+                root,
+                download=True,
+                transform=self.transform,
+                target_transform=ToUnknown(),
+                train=False,
             ),
             FashionMNIST(
-                root, download=True, transform=self.transform, target_transform=ToUnknown(), train=False
+                root,
+                download=True,
+                transform=self.transform,
+                target_transform=ToUnknown(),
+                train=False,
             ),
             Textures(
-                root, download=True, transform=self.transform, target_transform=ToUnknown()
+                root,
+                download=True,
+                transform=self.transform,
+                target_transform=ToUnknown(),
             ),
             Places365(
-                root, download=True, transform=self.transform, target_transform=ToUnknown()
-            )
+                root,
+                download=True,
+                transform=self.transform,
+                target_transform=ToUnknown(),
+            ),
         ]
 
         self.ood_names: List[str] = []  #: OOD Dataset names
         self.ood_names = [type(d).__name__ for d in self.test_oods]
 
     def train_set(self) -> Dataset:
         """
@@ -192,15 +231,15 @@
 
         if not known and unknown:
             return self.test_oods
 
         raise ValueError()
 
     def evaluate(
-            self, detector: Detector, loader_kwargs: Dict = None, device: str = "cpu"
+        self, detector: Detector, loader_kwargs: Dict = None, device: str = "cpu"
     ) -> List[Dict]:
         """
         Evaluates the given detector on all datasets and returns a list with the results
 
         :param detector: the detector to evaluate
         :param loader_kwargs: keyword arguments to give to the data loader
         :param device: the device to move batches to
```

## pytorch_ood/benchmark/img/cifar100.py

```diff
@@ -1,21 +1,30 @@
 """
 
 """
 from typing import Dict, List
 
 from torch.utils.data import DataLoader, Dataset
-from torchvision.datasets import CIFAR100, MNIST, CIFAR10, FashionMNIST
+from torchvision.datasets import CIFAR10, CIFAR100, MNIST, FashionMNIST
 from torchvision.transforms import Compose
 
 from pytorch_ood.api import Detector
-from pytorch_ood.dataset.img import LSUNCrop, LSUNResize, TinyImageNetCrop, TinyImageNetResize, GaussianNoise, \
-    UniformNoise, TinyImageNet, Places365, Textures
-from pytorch_ood.utils import OODMetrics, ToUnknown, ToRGB
 from pytorch_ood.benchmark import Benchmark
+from pytorch_ood.dataset.img import (
+    GaussianNoise,
+    LSUNCrop,
+    LSUNResize,
+    Places365,
+    Textures,
+    TinyImageNet,
+    TinyImageNetCrop,
+    TinyImageNetResize,
+    UniformNoise,
+)
+from pytorch_ood.utils import OODMetrics, ToRGB, ToUnknown
 
 
 class CIFAR100_ODIN(Benchmark):
     """
     Replicates the OOD detection benchmark from the ODIN paper for CIFAR 100.
 
     :see Paper: `ArXiv <https://arxiv.org/abs/1706.02690>`__
@@ -42,26 +51,39 @@
         self.test_oods = [
             TinyImageNetCrop(
                 root, download=True, transform=transform, target_transform=ToUnknown()
             ),
             TinyImageNetResize(
                 root, download=True, transform=transform, target_transform=ToUnknown()
             ),
-            LSUNResize(
-                root, download=True, transform=transform, target_transform=ToUnknown()
-            ),
-            LSUNCrop(
-                root, download=True, transform=transform, target_transform=ToUnknown()
+            LSUNResize(root, download=True, transform=transform, target_transform=ToUnknown()),
+            LSUNCrop(root, download=True, transform=transform, target_transform=ToUnknown()),
+            UniformNoise(
+                1000,
+                size=(32, 32, 3),
+                transform=transform,
+                target_transform=ToUnknown(),
+            ),
+            GaussianNoise(
+                1000,
+                size=(32, 32, 3),
+                transform=transform,
+                target_transform=ToUnknown(),
             ),
-            UniformNoise(1000, size=(32, 32, 3), transform=transform, target_transform=ToUnknown()),
-            GaussianNoise(1000, size=(32, 32, 3), transform=transform, target_transform=ToUnknown())
         ]
 
         self.ood_names: List[str] = []  #: OOD Dataset names
-        self.ood_names = ["TinyImageNetCrop", "TinyImageNetResize", "LSUNResize", "LSUNCrop", "Uniform", "Gaussian"]
+        self.ood_names = [
+            "TinyImageNetCrop",
+            "TinyImageNetResize",
+            "LSUNResize",
+            "LSUNCrop",
+            "Uniform",
+            "Gaussian",
+        ]
 
     def train_set(self) -> Dataset:
         """
         Training dataset
         """
         return self.train_in
 
@@ -143,31 +165,53 @@
         """
         self.transform = Compose([ToRGB(), transform])
         self.train_in = CIFAR100(root, download=True, transform=transform, train=True)
         self.test_in = CIFAR100(root, download=True, transform=transform, train=False)
 
         self.test_oods = [
             CIFAR10(
-                root, download=True, transform=self.transform, target_transform=ToUnknown(), train=False
+                root,
+                download=True,
+                transform=self.transform,
+                target_transform=ToUnknown(),
+                train=False,
             ),
             TinyImageNet(
-                root, download=True, transform=self.transform, target_transform=ToUnknown(), subset="test"
+                root,
+                download=True,
+                transform=self.transform,
+                target_transform=ToUnknown(),
+                subset="test",
             ),
             MNIST(
-                root, download=True, transform=self.transform, target_transform=ToUnknown(), train=False
+                root,
+                download=True,
+                transform=self.transform,
+                target_transform=ToUnknown(),
+                train=False,
             ),
             FashionMNIST(
-                root, download=True, transform=self.transform, target_transform=ToUnknown(), train=False
+                root,
+                download=True,
+                transform=self.transform,
+                target_transform=ToUnknown(),
+                train=False,
             ),
             Textures(
-                root, download=True, transform=self.transform, target_transform=ToUnknown()
+                root,
+                download=True,
+                transform=self.transform,
+                target_transform=ToUnknown(),
             ),
             Places365(
-                root, download=True, transform=self.transform, target_transform=ToUnknown()
-            )
+                root,
+                download=True,
+                transform=self.transform,
+                target_transform=ToUnknown(),
+            ),
         ]
 
         self.ood_names: List[str] = []  #: OOD Dataset names
         self.ood_names = [type(d).__name__ for d in self.test_oods]
 
     def train_set(self) -> Dataset:
         """
@@ -192,15 +236,15 @@
 
         if not known and unknown:
             return self.test_oods
 
         raise ValueError()
 
     def evaluate(
-            self, detector: Detector, loader_kwargs: Dict = None, device: str = "cpu"
+        self, detector: Detector, loader_kwargs: Dict = None, device: str = "cpu"
     ) -> List[Dict]:
         """
         Evaluates the given detector on all datasets and returns a list with the results
 
         :param detector: the detector to evaluate
         :param loader_kwargs: keyword arguments to give to the data loader
         :param device: the device to move batches to
```

## pytorch_ood/benchmark/img/imagenet.py

```diff
@@ -1,17 +1,17 @@
-from typing import List, Dict
+from typing import Dict, List
 
-from torch.utils.data import Dataset, DataLoader
+from torch.utils.data import DataLoader, Dataset
+from torchvision.datasets import MNIST, SVHN, ImageNet
 from torchvision.transforms import Compose
-from torchvision.datasets import ImageNet, MNIST, SVHN
 
 from pytorch_ood.api import Detector
 from pytorch_ood.benchmark import Benchmark
-from pytorch_ood.dataset.img import Textures, OpenImagesO, ImageNetO
-from pytorch_ood.utils import ToUnknown, ToRGB, OODMetrics
+from pytorch_ood.dataset.img import ImageNetO, OpenImagesO, Textures
+from pytorch_ood.utils import OODMetrics, ToRGB, ToUnknown
 
 
 class ImageNet_OpenOOD(Benchmark):
     """
     Aims to replicate the ImageNet benchmark proposed in
     *OpenOOD: Benchmarking Generalized Out-of-Distribution Detection*.
 
@@ -38,21 +38,46 @@
         """
         self.transform = Compose([ToRGB(), transform])
         self._train_in = None
         self.image_net_root = image_net_root
         self.test_in = ImageNet(image_net_root, transform=self.transform, split="val")
 
         self.test_oods = [
-            ImageNetO(root, download=True, transform=self.transform, target_transform=ToUnknown()),
-            OpenImagesO(root, download=True, transform=self.transform, target_transform=ToUnknown()),
+            ImageNetO(
+                root,
+                download=True,
+                transform=self.transform,
+                target_transform=ToUnknown(),
+            ),
+            OpenImagesO(
+                root,
+                download=True,
+                transform=self.transform,
+                target_transform=ToUnknown(),
+            ),
             Textures(
-                root, download=True, transform=self.transform, target_transform=ToUnknown()
+                root,
+                download=True,
+                transform=self.transform,
+                target_transform=ToUnknown(),
+            ),
+            SVHN(
+                root,
+                split="test",
+                download=True,
+                transform=self.transform,
+                target_transform=ToUnknown(),
+            ),
+            MNIST(
+                root,
+                root,
+                download=True,
+                transform=self.transform,
+                target_transform=ToUnknown(),
             ),
-            SVHN(root, split="test", download=True, transform=self.transform, target_transform=ToUnknown()),
-            MNIST(root, root, download=True, transform=self.transform, target_transform=ToUnknown())
         ]
 
         self.ood_names: List[str] = []  #: OOD Dataset names
         self.ood_names = [type(d).__name__ for d in self.test_oods]
 
     @property
     def train_in(self):
@@ -85,15 +110,15 @@
 
         if not known and unknown:
             return self.test_oods
 
         raise ValueError()
 
     def evaluate(
-            self, detector: Detector, loader_kwargs: Dict = None, device: str = "cpu"
+        self, detector: Detector, loader_kwargs: Dict = None, device: str = "cpu"
     ) -> List[Dict]:
         """
         Evaluates the given detector on all datasets and returns a list with the results
 
         :param detector: the detector to evaluate
         :param loader_kwargs: keyword arguments to give to the data loader
         :param device: the device to move batches to
```

## pytorch_ood/dataset/img/__init__.py

```diff
@@ -151,23 +151,34 @@
     :members:
 
 MVTech-AD
 `````````````
 ..  autoclass:: pytorch_ood.dataset.img.MVTechAD
     :members:
 
+
+Object Detection
+----------------------
+
+SuMNIST
+`````````````
+..  autoclass:: pytorch_ood.dataset.img.SuMNIST
+    :members:
+
+
 """
 from .chars74k import Chars74k
 from .cifar import CIFAR10C, CIFAR100C
 from .fooling import FoolingImages
 from .imagenet import ImageNetA, ImageNetC, ImageNetO, ImageNetR
 from .mnistc import MNISTC
 from .mvtech import MVTechAD
+from .ninco import NINCO
 from .noise import GaussianNoise, UniformNoise
 from .odin import LSUNCrop, LSUNResize, TinyImageNetCrop, TinyImageNetResize
+from .openood import OpenImagesO, Places365, iNaturalist
 from .pixmix import FeatureVisDataset, FractalDataset, PixMixDataset
 from .streethazards import StreetHazards
+from .sumnist import SuMNIST
 from .textures import Textures
 from .tinyimagenet import TinyImageNet
 from .tinyimages import TinyImages, TinyImages300k
-from .openood import OpenImagesO, iNaturalist, Places365
-from .ninco import NINCO
```

## pytorch_ood/dataset/img/base.py

```diff
@@ -1,15 +1,15 @@
 import logging
 import os
+from os.path import dirname, join
 from typing import Any, Callable, Optional, Tuple
 
 from PIL import Image
 from torchvision.datasets import VisionDataset
 from torchvision.datasets.utils import check_integrity, download_and_extract_archive
-from os.path import join, dirname
 
 log = logging.getLogger(__name__)
 
 
 def _get_resource_file(name):
     return join(dirname(__file__), "resources", name)
```

## pytorch_ood/dataset/img/ninco.py

```diff
@@ -22,37 +22,44 @@
 
 
     :see Paper: `ICML <https://arxiv.org/pdf/2306.00826.pdf>`__
     :see Code: `GitHub <https://github.com/j-cb/NINCO>`__
     :see Download: `Zenodo <https://zenodo.org/record/8013288>`__
 
     """
-    base_folders = ["NINCO_OOD_classes"] # , "NINCO_OOD_unit_tests", "NINCO_popular_datasets_subsamples"
+
+    base_folders = [
+        "NINCO_OOD_classes"
+    ]  # , "NINCO_OOD_unit_tests", "NINCO_popular_datasets_subsamples"
     url = "https://zenodo.org/record/8013288/files/NINCO_all.tar.gz"
     filename = "NINCO_all.tar.gz"
     tgz_md5s = "b9ffae324363cd900a81ce3c367cd834"
 
     def __init__(
         self,
         root: str,
         transform: Optional[Callable] = None,
         target_transform: Optional[Callable] = None,
         download: bool = False,
     ) -> None:
         super(NINCO, self).__init__(
-            root, transform=transform, target_transform=target_transform, download=download
+            root,
+            transform=transform,
+            target_transform=target_transform,
+            download=download,
         )
 
     def _load_files(self):
         files = []
         for d in self.base_folders:
             path = join(self.root, "NINCO", d)
             for subdir in os.listdir(path):
                 files += [join(path, subdir, img) for img in os.listdir(join(path, subdir))]
 
         return files
 
+
 if __name__ == "__main__":
     d = NINCO(root="/home/ki/datasets/", download=True)
     for i in range(len(d)):
         print(d[i])
-    print(len(d))
+    print(len(d))
```

## pytorch_ood/dataset/img/openood.py

```diff
@@ -1,19 +1,18 @@
 """
 Some of the datasets used in OpenOOD 1.5 benchmark.
 
 """
 import json
+import logging
 import os
+from os.path import dirname, exists, join
 from typing import Callable, Optional
 
-from os.path import exists, join, dirname
 from torchvision.datasets.utils import extract_archive
-import logging
-
 
 from pytorch_ood.dataset.img.base import ImageDatasetBase, _get_resource_file
 
 log = logging.getLogger(__name__)
 
 
 class iNaturalist(ImageDatasetBase):
@@ -24,37 +23,43 @@
     All labels are -1 by default.
 
     :see Paper: `MOS <https://arxiv.org/pdf/2105.01879.pdf>`__
     :see Paper: `iNaturalist <https://openaccess.thecvf.com/content_cvpr_2018/html/Van_Horn_The_INaturalist_Species_CVPR_2018_paper.html>`__
 
 
     """
+
     gdrive_id = "1zfLfMvoUD0CUlKNnkk7LgxZZBnTBipdj"
     filename = "iNaturalist.zip"
     target_dir = "iNaturalist"
     base_folder = join(target_dir, "images")
 
-    def __init__(self,
-                 root: str,
-                 transform: Optional[Callable] = None,
-                 target_transform: Optional[Callable] = None,
-                 download: bool = False,
-                 ) -> None:
-
+    def __init__(
+        self,
+        root: str,
+        transform: Optional[Callable] = None,
+        target_transform: Optional[Callable] = None,
+        download: bool = False,
+    ) -> None:
         self.archive_file = join(root, self.filename)
-        super(iNaturalist, self).__init__(root=root, transform=transform, target_transform=target_transform,
-                                          download=download)
+        super(iNaturalist, self).__init__(
+            root=root,
+            transform=transform,
+            target_transform=target_transform,
+            download=download,
+        )
 
     def download(self) -> None:
         if self._check_integrity():
             log.debug("Files already downloaded and verified")
             return
 
         try:
             import gdown
+
             gdown.download(id=self.gdrive_id, output=self.archive_file)
         except ImportError:
             raise RuntimeError("You have to install 'gdown' to download this dataset")
 
         extract_archive(from_path=self.archive_file, to_path=join(self.root, self.target_dir))
 
     def _check_integrity(self) -> bool:
@@ -67,37 +72,43 @@
     *OpenOOD: Benchmarking Generalized Out-of-Distribution Detection*.
     All labels are -1 by default.
 
     :see Website: `OpenImages <https://storage.googleapis.com/openimages/web/index.html>`__
 
     The test set contains 15869 , the validation set 1763 images.
     """
+
     gdrive_id = "1VUFXnB_z70uHfdgJG2E_pjYOcEgqM7tE"
     filename = "openimage_o.zip"
     target_dir = "OpenImagesO"
     base_folder = join(target_dir, "images")
 
     inclusion_json = {
         "test": "test_openimage_o.json",
         "val": "val_openimage_o.json",
     }
 
-    def __init__(self,
-                 root: str,
-                 subset="test",
-                 transform: Optional[Callable] = None,
-                 target_transform: Optional[Callable] = None,
-                 download: bool = False,
-                 ) -> None:
+    def __init__(
+        self,
+        root: str,
+        subset="test",
+        transform: Optional[Callable] = None,
+        target_transform: Optional[Callable] = None,
+        download: bool = False,
+    ) -> None:
         """
         :param subset: can be either ``val`` or ``test``
         """
         assert subset in list(self.inclusion_json.keys())
-        super(OpenImagesO, self).__init__(root=root, transform=transform, target_transform=target_transform,
-                                          download=download)
+        super(OpenImagesO, self).__init__(
+            root=root,
+            transform=transform,
+            target_transform=target_transform,
+            download=download,
+        )
 
         p = _get_resource_file(self.inclusion_json[subset])
         with open(p, "r") as f:
             included = json.load(f)
 
         self.files = [join(self.basedir, f) for f in included]
 
@@ -112,27 +123,33 @@
     :see Website: `Places <http://places.csail.mit.edu/browser.html>`__
 
     .. image:: https://production-media.paperswithcode.com/datasets/Places-0000003475-4b6da14b.jpg
       :target: http://places.csail.mit.edu/browser.html
       :alt: Places 365 examples
 
     """
+
     gdrive_id = "1Ec-LRSTf6u5vEctKX9vRp9OA6tqnJ0Ay"
     filename = "places365.zip"
     target_dir = "places365"
     base_folder = target_dir
 
-    def __init__(self,
-                 root: str,
-                 transform: Optional[Callable] = None,
-                 target_transform: Optional[Callable] = None,
-                 download: bool = False,
-                 ) -> None:
-        super(Places365, self).__init__(root=root, transform=transform, target_transform=target_transform,
-                                          download=download)
+    def __init__(
+        self,
+        root: str,
+        transform: Optional[Callable] = None,
+        target_transform: Optional[Callable] = None,
+        download: bool = False,
+    ) -> None:
+        super(Places365, self).__init__(
+            root=root,
+            transform=transform,
+            target_transform=target_transform,
+            download=download,
+        )
 
         self.files = []
 
         for d in os.listdir(self.basedir):
             p = join(self.basedir, d)
             if not os.path.isdir(join(p)):
                 continue
```

## pytorch_ood/dataset/img/pixmix.py

```diff
@@ -219,15 +219,16 @@
             if not gdown.download(id=self.google_drive_id, output=archive):
                 raise Exception("File must be downloaded manually")
 
             log.info("Extracting {archive} to {self.root}")
             extract_archive(archive, self.root, remove_finished=False)
 
         except ImportError:
-            raise ImportError(f"You have to install 'gdown' to use this dataset.")
+            raise ImportError("You have to install 'gdown' to use this dataset.")
+
 
 class FeatureVisDataset(PixMixExampleDatasets):
     """
     Dataset with Feature visualizations, as used in
     *PixMix: Dreamlike Pictures Comprehensively Improve Safety Measures*.
 
     :see Paper: `ArXiv <https://arxiv.org/abs/2112.05135>`__
@@ -241,15 +242,15 @@
         download=False,
     ) -> None:
         super(FeatureVisDataset, self).__init__(
             root,
             subset="features",
             transform=transform,
             target_transform=target_transform,
-            download=download
+            download=download,
         )
 
 
 class FractalDataset(PixMixExampleDatasets):
     """
     Dataset with Fractals, as used in
     *PixMix: Dreamlike Pictures Comprehensively Improve Safety Measures*.
@@ -258,16 +259,16 @@
     """
 
     def __init__(
         self,
         root: str,
         transform: Optional[Callable] = None,
         target_transform: Optional[Callable] = None,
-        download=False
+        download=False,
     ) -> None:
         super(FractalDataset, self).__init__(
             root,
             subset="fractals",
             transform=transform,
             target_transform=target_transform,
-            download=download
+            download=download,
         )
```

## pytorch_ood/dataset/img/tinyimagenet.py

```diff
@@ -1,16 +1,15 @@
 import logging
 import os
 from os.path import exists, join
-from PIL import Image
 
+from PIL import Image
 from torchvision.datasets import VisionDataset
 from torchvision.datasets.utils import download_and_extract_archive
 
-
 log = logging.getLogger(__name__)
 
 
 class TinyImageNet(VisionDataset):
     """
     Small Version of the ImageNet with images of size :math:`64 \\times 64` from 200 classes used by
     Stanford. Each class has 500 images for training.
@@ -29,15 +28,22 @@
 
     url = "http://cs231n.stanford.edu/tiny-imagenet-200.zip"
     dir_name = "tiny-imagenet-200"
     tgz_md5 = "90528d7ca1a48142e341f4ef8d21d0de"
     filename = "tiny-imagenet-200.zip"
     subsets = ["train", "val", "test"]
 
-    def __init__(self, root, subset="train", download=False, transform=None, target_transform=None):
+    def __init__(
+        self,
+        root,
+        subset="train",
+        download=False,
+        transform=None,
+        target_transform=None,
+    ):
         """
         :para subset: can be one of ``train``, ``val`` and ``test``
         """
         if subset not in self.subsets:
             raise ValueError(f"Invalid subset: {subset}. Possible values are {self.subsets}")
 
         super(TinyImageNet, self).__init__(
@@ -109,8 +115,7 @@
         if self.target_transform is not None:
             target = self.target_transform(target)
 
         return img, target
 
     def __len__(self):
         return len(self.paths)
-
```

## pytorch_ood/detector/__init__.py

```diff
@@ -58,14 +58,15 @@
 .. automodule:: pytorch_ood.detector.klmatching
 
 Entropy
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 .. automodule:: pytorch_ood.detector.entropy
 
 
+
 Logit-based
 -------------------------------
 
 Maximum Logit
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 .. automodule:: pytorch_ood.detector.maxlogit
 
@@ -73,14 +74,19 @@
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 .. automodule:: pytorch_ood.detector.openmax
 
 Energy Based (EBO)
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 .. automodule:: pytorch_ood.detector.energy
 
+Weighted Energy Based (WEBO)
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+.. automodule:: pytorch_ood.detector.webo
+
+
 ODIN Preprocessing
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 .. automodule:: pytorch_ood.detector.odin
 
 Feature-based
 -------------------------------
 
@@ -120,26 +126,28 @@
 ^^^^^^^^^^^^^^^^^^^^^^^^^
 .. automodule:: pytorch_ood.detector.react
 
 DICE
 ^^^^^^^^^^^^^^^^^^^^^^^^^
 .. automodule:: pytorch_ood.detector.dice
 
+
+
 """
+from .ash import ASH
+from .dice import DICE
 from .energy import EnergyBased
 from .entropy import Entropy
 from .klmatching import KLMatching
+from .knn import KNN
 from .mahalanobis import Mahalanobis
 from .maxlogit import MaxLogit
 from .mcd import MCD
 from .odin import ODIN, odin_preprocessing
 from .openmax import OpenMax
-from .softmax import MaxSoftmax
-from .tscaling import TemperatureScaling
-from .vim import ViM
-from .knn import KNN
-from .ash import ASH
 from .react import ReAct
 from .rmd import RMD
-from .dice import DICE
 from .she import SHE
-
+from .softmax import MaxSoftmax
+from .tscaling import TemperatureScaling
+from .vim import ViM
+from .webo import WeightedEBO
```

## pytorch_ood/detector/ash.py

```diff
@@ -5,24 +5,23 @@
 .. image:: https://img.shields.io/badge/segmentation-yes-brightgreen?style=flat-square
    :alt: segmentation badge
 
 ..  autoclass:: pytorch_ood.detector.ASH
     :members:
     :exclude-members: fit, fit_features, predict_features
 """
-from typing import TypeVar, Callable
+import logging
+from typing import Callable, TypeVar
 
+import numpy as np
 import torch.nn
 from torch import Tensor
 
-import logging
-import numpy as np
-
 from ..api import Detector
-from pytorch_ood.detector import EnergyBased
+from .energy import EnergyBased
 
 log = logging.getLogger(__name__)
 Self = TypeVar("Self")
 
 
 def ash_b(x: Tensor, percentile: float = 0.65) -> Tensor:
     assert x.dim() == 4
@@ -112,16 +111,22 @@
 
     variants = {
         "ash-s": ash_s,
         "ash-p": ash_p,
         "ash-b": ash_b,
     }
 
-    def __init__(self, backbone: Callable[[Tensor], Tensor], head: Callable[[Tensor], Tensor],
-                 variant="ash-s", percentile: float = 0.65, detector: Callable[[Tensor], Tensor] = None):
+    def __init__(
+        self,
+        backbone: Callable[[Tensor], Tensor],
+        head: Callable[[Tensor], Tensor],
+        variant="ash-s",
+        percentile: float = 0.65,
+        detector: Callable[[Tensor], Tensor] = None,
+    ):
         """
         :param variant: one of ``ash-p``, ``ash-b``, ``ash-s``
         :param backbone: first part of model to use, should output feature maps
         :param head: second part of model used after applying ash, should output logits
         :param percentile: amount of activations to modify
         :param detector: detector that maps model outputs to outlier scores. Default is Energy based.
         """
```

## pytorch_ood/detector/dice.py

```diff
@@ -5,40 +5,47 @@
 .. image:: https://img.shields.io/badge/segmentation-no-red?style=flat-square
    :alt: segmentation badge
 
 ..  autoclass:: pytorch_ood.detector.DICE
     :members:
 
 """
-from typing import TypeVar, Callable
+import logging
+from typing import Callable, TypeVar
 
+import numpy as np
 import torch.nn
 from torch import Tensor
 from torch.utils.data import DataLoader
-import logging
-import numpy as np
 
-from ..api import Detector
 from pytorch_ood.utils import extract_features, is_known
-from pytorch_ood.detector import EnergyBased
+
+from ..api import Detector
+from .energy import EnergyBased
 
 log = logging.getLogger(__name__)
 Self = TypeVar("Self")
 
 
 class DICE(Detector):
     """
     Implements DICE from the paper
     *DICE: Leveraging Sparsification for Out-of-Distribution Detection*.
 
     :see Paper: `ArXiv <https://arxiv.org/abs/2111.09805>`__
     """
 
-    def __init__(self, model: Callable[[Tensor], Tensor], w: torch.Tensor, b: torch.Tensor, p: float,
-                 detector: Callable[[Tensor], Tensor] = None):
+    def __init__(
+        self,
+        model: Callable[[Tensor], Tensor],
+        w: torch.Tensor,
+        b: torch.Tensor,
+        p: float,
+        detector: Callable[[Tensor], Tensor] = None,
+    ):
         """
         :param model: feature extractor
         :param w: weights of last layer
         :param b: bias of last layer
         :param p: percentile of weights to drop
         """
         self.model = model
@@ -75,15 +82,15 @@
 
         :param z: features
         :param y: labels.
         """
         known = is_known(y)
 
         if not known.any():
-            raise ValueError(f"No IN data")
+            raise ValueError("No IN data")
 
         z = z[known]
 
         self.mean_activation = z.mean(dim=0)
 
         contrib = self.mean_activation[None, :] * self.weight
         self.threshold = np.percentile(contrib, self.percentile)
```

## pytorch_ood/detector/knn.py

```diff
@@ -6,20 +6,20 @@
    :alt: segmentation badge
 
 ..  autoclass:: pytorch_ood.detector.KNN
     :members:
 
 """
 import logging
-from typing import TypeVar, Callable
+from typing import Callable, TypeVar
 
 from torch import Tensor, tensor
 from torch.utils.data import DataLoader
 
-from pytorch_ood.api import RequiresFittingException, Detector, ModelNotSetException
+from pytorch_ood.api import Detector, ModelNotSetException, RequiresFittingException
 from pytorch_ood.utils import extract_features, is_known
 
 log = logging.getLogger(__name__)
 Self = TypeVar("Self")
 
 
 class KNN(Detector):
@@ -34,26 +34,27 @@
 
     where :math:`\\mathcal{D}` is the dataset used to train the nearest neighbor model.
 
     The original paper found that using contrastive pre-training could increase the performance.
 
     :see PMLR: `arXiv <https://proceedings.mlr.press/v162/sun22d.html>`__
     """
+
     def __init__(self, model: Callable[[Tensor], Tensor], **knn_kwargs):
         """
         :param model: neural network to use
         :param knn_kwargs: dict with keyword arguments that will be passed to the scikit learns k-NN
         """
         self.model = model
         self._is_fitted = False
 
         try:
             from sklearn.neighbors import NearestNeighbors
         except ImportError:
-            raise Exception(f"You have to install scikit-learn to use this detector")
+            raise Exception("You have to install scikit-learn to use this detector")
 
         self.knn: NearestNeighbors = NearestNeighbors(n_neighbors=1, n_jobs=-1, **knn_kwargs)
 
     def predict(self, x: Tensor) -> Tensor:
         """
         :param x: inputs, will be passed through model
         """
@@ -68,29 +69,31 @@
         :param z: features
         :param k: number of neighbors
         """
 
         if not self._is_fitted:
             raise RequiresFittingException()
 
-        dist, idx = self.knn.kneighbors(z.detach().cpu().numpy(), n_neighbors=1, return_distance=True)
+        dist, idx = self.knn.kneighbors(
+            z.detach().cpu().numpy(), n_neighbors=1, return_distance=True
+        )
 
         return tensor(dist)
 
     def fit_features(self: Self, z: Tensor, labels: Tensor) -> Self:
         """
         Fits nearest neighbor model. Ignores OOD inputs.
 
         :param z: features
         :param labels: labels for features
         """
         known = is_known(labels)
 
         if not known.any():
-            raise ValueError(f"No IN samples")
+            raise ValueError("No IN samples")
 
         self.knn.fit(z[known].numpy())
 
         self._is_fitted = True
 
         return self
```

## pytorch_ood/detector/mahalanobis.py

```diff
@@ -39,18 +39,18 @@
     Also uses ODIN preprocessing.
 
     :see Implementation: `GitHub <https://github.com/pokaxpoka/deep_Mahalanobis_detector>`__
     :see Paper: `ArXiv <https://arxiv.org/abs/1807.03888>`__
     """
 
     def __init__(
-            self,
-            model: Callable[[Tensor], Tensor],
-            eps: float = 0.002,
-            norm_std: Optional[List] = None,
+        self,
+        model: Callable[[Tensor], Tensor],
+        eps: float = 0.002,
+        norm_std: Optional[List] = None,
     ):
         """
         :param model: the Neural Network, should output features
         :param eps: magnitude for gradient based input preprocessing
         :param norm_std: Standard deviations for input normalization
         """
         super(Mahalanobis, self).__init__()
@@ -110,17 +110,15 @@
             self.cov += (zs - self.mu[clazz]).T.mm(zs - self.mu[clazz])
 
         self.cov += torch.eye(self.cov.shape[0], device=self.cov.device) * 1e-6
         self.precision = torch.linalg.inv(self.cov)
         return self
 
     def _calc_gaussian_scores(self, z: Tensor) -> Tensor:
-        """
-
-        """
+        """ """
         features = z.view(z.size(0), z.size(1), -1)
         features = torch.mean(features, 2)
         md_k = []
 
         # calculate per class scores
         for clazz in range(self.n_classes):
             centered_z = features.data - self.mu[clazz]
@@ -136,15 +134,15 @@
 
         :param z: features, as given by the model.
         """
         if self.mu is None:
             raise RequiresFittingException
 
         md_k = self._calc_gaussian_scores(z)
-        score = - torch.max(md_k, dim=1).values
+        score = -torch.max(md_k, dim=1).values
         return score
 
     def predict(self, x: Tensor) -> Tensor:
         """
         :param x: input tensor
         """
         if self.model is None:
@@ -173,37 +171,37 @@
                 features = self.model(x)
                 features = features.view(features.shape[0], -1)  # flatten
                 score = None
 
                 for clazz in range(self.n_classes):
                     centered_features = features.data - self.mu[clazz]
                     term_gau = (
-                            -0.5
-                            * torch.mm(
-                        torch.mm(centered_features, self.precision),
-                        centered_features.t(),
-                    ).diag()
+                        -0.5
+                        * torch.mm(
+                            torch.mm(centered_features, self.precision),
+                            centered_features.t(),
+                        ).diag()
                     )
 
                     if clazz == 0:
                         score = term_gau.view(-1, 1)
                     else:
                         score = torch.cat((score, term_gau.view(-1, 1)), dim=1)
 
                 # calculate gradient of inputs with respect to score of predicted class,
                 # according to mahalanobis distance
                 sample_pred = score.max(dim=1).indices
                 batch_sample_mean = self.mu.index_select(0, sample_pred)
                 centered_features = features - Variable(batch_sample_mean)
                 pure_gau = (
-                        -0.5
-                        * torch.mm(
-                    torch.mm(centered_features, Variable(self.precision)),
-                    centered_features.t(),
-                ).diag()
+                    -0.5
+                    * torch.mm(
+                        torch.mm(centered_features, Variable(self.precision)),
+                        centered_features.t(),
+                    ).diag()
                 )
                 loss = torch.mean(-pure_gau)
                 loss.backward()
 
                 gradient = torch.sign(x.grad.data)
 
         if self.norm_std:
```

## pytorch_ood/detector/mcd.py

```diff
@@ -7,15 +7,15 @@
 
 ..  autoclass:: pytorch_ood.detector.MCD
     :members:
     :exclude-members: predict_features, fit, fit_features
 
 """
 import logging
-from typing import TypeVar, Tuple
+from typing import Tuple, TypeVar
 
 import torch
 from torch import Tensor, nn
 from torch.nn import Module
 
 from ..api import Detector, ModelNotSetException
 
@@ -44,15 +44,21 @@
     :see Bayesian SegNet: `ArXiv <https://arxiv.org/abs/1511.02680>`__
 
     .. warning:: This implementations puts the model into evaluation mode (except for variants of the BatchNorm Layers).
         This could also affect other modules.
 
     """
 
-    def __init__(self, model: Module, samples: int = 30, mode: str = "var", batch_norm: bool = True):
+    def __init__(
+        self,
+        model: Module,
+        samples: int = 30,
+        mode: str = "var",
+        batch_norm: bool = True,
+    ):
         """
 
         :param model: the module to use for the forward pass. Should output logits.
         :param samples: number of iterations
         :param mode: can be one of ``var`` or ``mean``
         :param batch_norm: keep batch norm layers in evaluation mode
         """
@@ -185,8 +191,12 @@
         :param x: input
         :return: outlier score
         """
         if self.mode == "var":
             print("calculating variance")
             return MCD.run(self.model, x, self.n_samples, batch_norm=self.batch_norm)[1]
 
-        return -MCD.run_mean(self.model, x, self.n_samples, batch_norm=self.batch_norm).max(dim=1).values
+        return (
+            -MCD.run_mean(self.model, x, self.n_samples, batch_norm=self.batch_norm)
+            .max(dim=1)
+            .values
+        )
```

## pytorch_ood/detector/react.py

```diff
@@ -6,20 +6,21 @@
    :alt: segmentation badge
 
 ..  autoclass:: pytorch_ood.detector.ReAct
     :members:
     :exclude-members: fit, fit_features, predict_features
 
 """
-from typing import TypeVar, Callable
-from torch import Tensor
 import logging
+from typing import Callable, TypeVar
+
+from torch import Tensor
 
 from ..api import Detector
-from pytorch_ood.detector import EnergyBased
+from .energy import EnergyBased
 
 log = logging.getLogger(__name__)
 Self = TypeVar("Self")
 
 
 class ReAct(Detector):
     """
@@ -44,16 +45,21 @@
             detector = EnergyBased.score
         )
         scores = detector(images)
 
     :see Paper: `ArXiv <https://arxiv.org/abs/2111.12797>`__
     """
 
-    def __init__(self, backbone: Callable[[Tensor], Tensor], head: Callable[[Tensor], Tensor],
-                 threshold: float = 1.0, detector: Callable[[Tensor], Tensor] = None):
+    def __init__(
+        self,
+        backbone: Callable[[Tensor], Tensor],
+        head: Callable[[Tensor], Tensor],
+        threshold: float = 1.0,
+        detector: Callable[[Tensor], Tensor] = None,
+    ):
         """
         :param backbone: first part of model to use, should output feature maps
         :param head: second part of model used after applying ash, should output logits
         :param threshold: cutoff for activations
         :param detector: detector that maps outputs to outlier scores. Default is energy based.
         """
         self.backbone = backbone
```

## pytorch_ood/detector/rmd.py

```diff
@@ -14,17 +14,18 @@
 from typing import Callable, List, Optional, TypeVar
 
 import torch
 from torch import Tensor
 from torch.autograd import Variable
 from torch.utils.data import DataLoader
 
+from pytorch_ood.detector.mahalanobis import Mahalanobis
+
 from ..api import Detector, ModelNotSetException, RequiresFittingException
 from ..utils import TensorBuffer, contains_unknown, extract_features, is_known, is_unknown
-from pytorch_ood.detector.mahalanobis import Mahalanobis
 
 log = logging.getLogger(__name__)
 
 Self = TypeVar("Self")
 
 
 class RMD(Mahalanobis):
@@ -84,15 +85,17 @@
         known = is_known(y)
 
         super(RMD, self).fit_features(z, y, device)
 
         log.debug("Fitting background gaussian.")
         self.background_mu = z[known].mean(dim=0)
         self.background_cov = (z[known] - self.background_mu).T.mm(z[known] - self.background_mu)
-        self.background_cov += torch.eye(self.background_cov.shape[0], device=self.background_cov.device) * 1e-6
+        self.background_cov += (
+            torch.eye(self.background_cov.shape[0], device=self.background_cov.device) * 1e-6
+        )
 
         self.background_precision = torch.linalg.inv(self.background_cov)
         return self
 
     def _background_score(self, z: Tensor) -> Tensor:
         centered_z = z - self.background_mu
         return torch.mm(torch.mm(centered_z, self.background_precision), centered_z.t()).diag()
```

## pytorch_ood/detector/she.py

```diff
@@ -3,23 +3,24 @@
    :alt: classification badge
 .. image:: https://img.shields.io/badge/segmentation-no-red?style=flat-square
    :alt: classification badge
 
 ..  autoclass:: pytorch_ood.detector.SHE
     :members:
 """
-from typing import TypeVar, Callable
+from typing import Callable, TypeVar
 
 import torch
 from torch import Tensor
 from torch.utils.data import DataLoader
 
-from ..api import Detector, ModelNotSetException
 from pytorch_ood.utils import extract_features, is_known
 
+from ..api import Detector, ModelNotSetException
+
 Self = TypeVar("Self")
 
 
 class SHE(Detector):
     """
     Implements Simplified Hopfield Energy from the paper
     *Out-of-Distribution Detection based on In-Distribution Data Patterns Memorization with modern Hopfield Energy*
```

## pytorch_ood/detector/softmax.py

 * *Ordering differences only*

```diff
@@ -5,27 +5,28 @@
 .. image:: https://img.shields.io/badge/segmentation-yes-brightgreen?style=flat-square
    :alt: classification badge
 
 ..  autoclass:: pytorch_ood.detector.MaxSoftmax
     :members:
     :exclude-members: fit, fit_features
 """
+import logging
 from typing import Optional, TypeVar
 
 import torch.nn
 from torch import Tensor, tensor
-from torch.nn.functional import nll_loss
 from torch.nn import Module
+from torch.nn.functional import nll_loss
 from torch.optim import LBFGS
 from torch.utils.data import DataLoader
-import logging
 
-from ..api import Detector, ModelNotSetException, RequiresFittingException
 from pytorch_ood.utils import extract_features, is_known
 
+from ..api import Detector, ModelNotSetException, RequiresFittingException
+
 log = logging.getLogger(__name__)
 Self = TypeVar("Self")
 
 
 class MaxSoftmax(Detector):
     """
     Implements the Maximum Softmax Probability (MSP) Thresholding baseline for OOD detection.
@@ -84,8 +85,7 @@
     @staticmethod
     def score(logits: Tensor, t: Optional[float] = 1.0) -> Tensor:
         """
         :param logits: logits for samples
         :param t: temperature value
         """
         return -logits.div(t).softmax(dim=1).max(dim=1).values
-
```

## pytorch_ood/detector/tscaling.py

```diff
@@ -5,33 +5,34 @@
 .. image:: https://img.shields.io/badge/segmentation-no-red?style=flat-square
    :alt: segmentation badge
 
 ..  autoclass:: pytorch_ood.detector.TemperatureScaling
     :members:
 
 """
+import logging
 from typing import Optional, TypeVar
 
 import torch.nn
 from torch import Tensor, tensor
-from torch.nn.functional import nll_loss, log_softmax
 from torch.nn import Module
+from torch.nn.functional import log_softmax, nll_loss
 from torch.optim import LBFGS
 from torch.utils.data import DataLoader
-import logging
 
-from ..api import RequiresFittingException
 from pytorch_ood.detector.softmax import MaxSoftmax
 from pytorch_ood.utils import extract_features, is_known
 
+from ..api import RequiresFittingException
+
 log = logging.getLogger(__name__)
 Self = TypeVar("Self")
 
 
-class TemperatureScaling(MaxSoftmax,  torch.nn.Module):
+class TemperatureScaling(MaxSoftmax, torch.nn.Module):
     """
     Implements temperature scaling from the paper
     *On Calibration of Modern Neural Networks*.
 
     The method uses an additional set of validation samples to determine the optimal temperature
     value :math:`T` to calibrate the softmax output.
 
@@ -40,14 +41,15 @@
     .. math:: - \\max_y \\sigma_y(f(x) / T)
 
     where :math:`\\sigma` is the softmax function, :math:`T` is the optimal temperature and :math:`\\sigma_y`
     indicates the :math:`y^{th}` value of the resulting probability vector.
 
     :see Paper: `ArXiv <https://arxiv.org/pdf/1706.04599.pdf>`__
     """
+
     def __init__(self, model: Module):
         """
         :param model: neural network to use
         """
         super(TemperatureScaling, self).__init__(model=model)
         self.t = torch.nn.Parameter(tensor(1.0))
         self._is_fitted = False
@@ -67,15 +69,15 @@
 
         :param logits: logits
         :param labels: labels for logits
         """
         known = is_known(labels)
 
         if not known.any():
-            raise ValueError(f"No IN samples")
+            raise ValueError("No IN samples")
 
         optimizer = LBFGS([self.t], lr=0.01, max_iter=50)
 
         device = self.t.device
 
         logits = logits[known].to(device)
         labels = labels[known].to(device)
```

## pytorch_ood/detector/openmax/libnotmr.py

```diff
@@ -1,47 +1,52 @@
 """
 Using the weibull fitting implementation from
 https://github.com/ashafaei/OD-test/blob/8252aace84e2ae1ab95067876985f62a1060aad6/methods/openmax.py
 
 
 """
 
-import numpy as np
-from scipy.stats import exponweib
 import logging
+
+import numpy as np
 import scipy.stats
+from scipy.stats import exponweib
 
 log = logging.getLogger(__name__)
 
 
 class LibNotMR(object):
     """
-        Instead of using LibMR (https://github.com/abhijitbendale/OSDN/tree/master/libMR) we implemented
-        the simple operations with Scipy. The output is checked against the original library for verification.
+    Instead of using LibMR (https://github.com/abhijitbendale/OSDN/tree/master/libMR) we implemented
+    the simple operations with Scipy. The output is checked against the original library for verification.
     """
 
     def __init__(self, tailsize: int = 20):
         self.tailsize = tailsize
         self.min_val = None
         self.translation = 10000  # this constant comes from the library.
         # it only makes small numerical differences.
         # we keep it for authenticity.
         self.a = 1
         self.loc = 0
         self.c = None
         self.scale = None
 
     def fit_high(self, inputs: np.ndarray) -> None:
-        tailtofit = sorted(inputs)[-self.tailsize:]
+        tailtofit = sorted(inputs)[-self.tailsize :]
         self.min_val = np.min(tailtofit)
         new_inputs = [i + self.translation - self.min_val for i in tailtofit]
         params = exponweib.fit(new_inputs, floc=0, f0=1)
         self.c = params[1]
         self.scale = params[3]
 
     def w_score(self, inputs: np.ndarray) -> np.ndarray:
         new_inputs = inputs + self.translation - self.min_val
         new_score = exponweib.cdf(new_inputs, a=self.a, c=self.c, loc=self.loc, scale=self.scale)
         return new_score
 
     def __str__(self):
-        return 'Weib: C=%.2f scale=%.2f min_val=%.2f' % (self.c, self.scale, self.min_val)
+        return "Weib: C=%.2f scale=%.2f min_val=%.2f" % (
+            self.c,
+            self.scale,
+            self.min_val,
+        )
```

## pytorch_ood/detector/openmax/numpy.py

```diff
@@ -1,13 +1,15 @@
 import logging
 from typing import TypeVar
 
 import numpy as np
 import scipy.spatial.distance as distance
+
 from .libnotmr import LibNotMR
+
 Self = TypeVar("Self")
 
 log = logging.getLogger(__name__)
 
 
 class OpenMax(object):
     """
```

## pytorch_ood/loss/__init__.py

```diff
@@ -96,15 +96,15 @@
 
 
 Outlier Exposure Loss
 ----------------------------------------------
 
 .. image:: https://img.shields.io/badge/classification-yes-brightgreen?style=flat-square
    :alt: classification badge
-.. image:: https://img.shields.io/badge/segmentation-no-red?style=flat-square
+.. image:: https://img.shields.io/badge/segmentation-yes-brightgreen?style=flat-square
    :alt: classification badge
 
 .. autoclass:: pytorch_ood.loss.OutlierExposureLoss
     :members:
 
 
 Entropic Open-Set Loss
@@ -132,20 +132,31 @@
 
 
 Energy-Bounded Learning Loss
 ----------------------------------------------
 
 .. image:: https://img.shields.io/badge/classification-yes-brightgreen?style=flat-square
    :alt: classification badge
-.. image:: https://img.shields.io/badge/segmentation-no-red?style=flat-square
+.. image:: https://img.shields.io/badge/segmentation-yes-brightgreen?style=flat-square
    :alt: classification badge
 
 .. autoclass:: pytorch_ood.loss.EnergyRegularizedLoss
     :members:
 
+VOS Energy-Based Loss
+----------------------------------------------
+
+.. image:: https://img.shields.io/badge/classification-yes-brightgreen?style=flat-square
+   :alt: classification badge
+.. image:: https://img.shields.io/badge/segmentation-yes-brightgreen?style=flat-square
+   :alt: classification badge
+
+.. autoclass:: pytorch_ood.loss.VOSRegLoss
+    :members:
+
 
 MCHAD Loss
 ----------------------------------------------
 
 .. image:: https://img.shields.io/badge/classification-yes-brightgreen?style=flat-square
    :alt: classification badge
 .. image:: https://img.shields.io/badge/segmentation-no-red?style=flat-square
@@ -178,7 +189,8 @@
 from .ii import IILoss
 from .mchad import MCHADLoss
 from .objectosphere import ObjectosphereLoss
 from .oe import OutlierExposureLoss
 
 # from .triplet import TripletLoss
 from .svdd import DeepSVDDLoss, SSDeepSVDDLoss
+from .vos import VOSRegLoss
```

## pytorch_ood/loss/energy.py

```diff
@@ -35,16 +35,16 @@
 
     :see Implementation: `GitHub <https://github.com/wetliu/energy_ood>`__
     """
 
     def __init__(
         self,
         alpha: float = 1.0,
-        margin_in: float = 1.0,
-        margin_out: float = 1.0,
+        margin_in: float = -1.0,
+        margin_out: float = -1.0,
         reduction: str = "mean",
     ):
         """
         :param alpha: weighting parameter
         :param margin_in:  margin energy :math:`m_{in}` for IN data
         :param margin_out: margin energy :math:`m_{out}` for OOD data
         :param reduction: can be one of ``none``, ``mean``, ``sum``
@@ -65,15 +65,35 @@
         regularization = self._regularization(logits, targets)
         nll = cross_entropy(logits, targets, reduction="none")
         return apply_reduction(nll + self.alpha * regularization, reduction=self.reduction)
 
     def _regularization(self, logits: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
         energy = torch.zeros(logits.shape[0]).to(logits.device)
 
-        known = is_known(y)
-        if known.any():
-            energy[known] = (_energy(logits[is_known(y)]) - self.m_in).relu().pow(2)
+        # for classification
+        if len(logits.shape) == 2:
+            known = is_known(y)
 
-        if (~known).any():
             energy[~known] = (self.m_out - _energy(logits[is_unknown(y)])).relu().pow(2)
+            if known.any():
+                energy[known] = (_energy(logits[is_known(y)]) - self.m_in).relu().pow(2)
 
+            if (~known).any():
+                energy[~known] = (self.m_out - _energy(logits[is_unknown(y)])).relu().pow(2)
+
+        # for segmentation
+        elif len(logits.shape) == 4:
+            logits_form = logits.permute(0, 2, 3, 1)
+            if is_known(y).any():
+                energy_in = (_energy(logits_form[is_known(y)]) - self.m_in).relu().pow(2).mean()
+            else:
+                energy_in = 0
+            if is_unknown(y).any():
+                energy_out = (
+                    (_energy(self.m_out - logits_form[is_unknown(y)])).relu().pow(2).mean()
+                )
+            else:
+                energy_out = 0
+            energy = energy_in + energy_out
+        else:
+            raise ValueError(f"Unsupported input shape: {logits.shape}")
         return energy
```

## pytorch_ood/loss/oe.py

```diff
@@ -49,17 +49,36 @@
     def forward(self, logits: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
         """
 
         :param logits: class logits for predictions
         :param target: labels for predictions
         :return: loss
         """
-        loss_oe = torch.zeros(logits.shape[0], device=logits.device)
-        loss_ce = cross_entropy(logits, target, reduction=None)
 
-        if contains_unknown(target):
-            unknown = is_unknown(target)
-            loss_oe[unknown] = -(
-                logits[unknown].mean(dim=1) - torch.logsumexp(logits[unknown], dim=1)
-            )
+        # for classification
+        if len(logits.shape) == 2:
+            loss_oe = torch.zeros(logits.shape[0], device=logits.device)
+            loss_ce = cross_entropy(logits, target, reduction=None)
 
-        return apply_reduction(loss_ce + self.alpha * loss_oe, reduction=self.reduction)
+            if contains_unknown(target):
+                unknown = is_unknown(target)
+                loss_oe[unknown] = -(
+                    logits[unknown].mean(dim=1) - torch.logsumexp(logits[unknown], dim=1)
+                )
+
+            return apply_reduction(loss_ce + self.alpha * loss_oe, reduction=self.reduction)
+
+        # for segmentation
+        elif len(logits.shape) == 4:
+            loss_ce = cross_entropy(logits, target, reduction=None)
+            logits = logits.permute(0, 2, 3, 1)
+            if contains_unknown(target):
+                unknown = is_unknown(target)
+                loss_oe = -(logits[unknown].mean(dim=1) - torch.logsumexp(logits[unknown], dim=1))
+            else:
+                loss_oe = torch.zeros(logits.shape[:3], device=logits.device)
+
+            # print(loss_ce.shape)
+            # print(loss_oe.shape)
+            return apply_reduction(loss_ce + self.alpha * loss_oe, reduction=self.reduction)
+        else:
+            raise ValueError(f"Unsupported input shape: {logits.shape}")
```

## pytorch_ood/loss/svdd.py

```diff
@@ -81,15 +81,18 @@
         :return: :math:`\\lVert x - \\mu \\rVert^2 - r^2`
         """
         loss = DeepSVDDLoss.svdd_loss(x, self.center, radius=self.radius, y=y)
         return apply_reduction(loss, self.reduction)
 
     @staticmethod
     def svdd_loss(
-        x: Tensor, center: ClassCenters, radius: Tensor = 0.0, y: Optional[Tensor] = None
+        x: Tensor,
+        center: ClassCenters,
+        radius: Tensor = 0.0,
+        y: Optional[Tensor] = None,
     ) -> Tensor:
         """
         Calculates the loss. Treats all IN samples equally, and ignores all OOD samples.
         If no labels are given, assumes all samples are IN.
 
         :param x: features
         :param center: center of sphere
```

## pytorch_ood/utils/metrics.py

```diff
@@ -115,15 +115,15 @@
 
         metrics = OODMetrics()
         outlier_scores = torch.Tensor([0.5, 1.0, -10])
         labels = torch.Tensor([1,2,-1])
         metrics.update(outlier_scores, labels)
         metric_dict = metrics.compute()
 
-    In ``classification`` mode, the inputs will be flattened, so we treat each value as an individual text example.
+    In ``classification`` mode, the inputs will be flattened, so we treat each value as an individual example.
     Using this mode for segmentation tasks can require a lot of memory and compute.
 
     In ``segmentation`` mode, the inputs will be flattened along the first (batch) dimension so that the shape is
     :math:`B \\times D` afterwards.
     The scores will then be calculated for each sample in the batch (i.e., over :math:`D` values each), and the final
     score will be the mean over all :math:`B` samples.
     """
@@ -131,44 +131,50 @@
     def __init__(self, device: str = "cpu", mode: str = "classification"):
         """
         :param device: where tensors should be stored
         :param mode: either ``classification`` or ``segmentation``.
         """
         super(OODMetrics, self).__init__()
         self.device = device
-        self.buffer = TensorBuffer(device=self.device)
+        # always buffer on cpu to not exhaust gpu mem
+        self.buffer = TensorBuffer(device="cpu")
 
         if mode not in ["segmentation", "classification"]:
             raise ValueError("mode must be 'segmentation' or 'classification'")
 
         self.mode = mode
 
     def update(self: Self, scores: Tensor, y: Tensor) -> Self:
         """
         Add batch of results to collection.
 
         :param scores: outlier score
         :param y: target label
         """
-        label = is_unknown(y).detach().to(self.device).long()
-        scores = scores.detach().to(self.device)
+        label = is_unknown(y).detach().long()
 
         if self.mode == "classification":
             self.buffer.append("scores", scores)
             self.buffer.append("labels", label)
 
         elif self.mode == "segmentation":
+            assert scores.device == y.device, "Score and target tensor must be on same device"
+
             # loop along batch dimension
             for i in range(scores.shape[0]):
+                # computation will be carried out on the device where the data currently resides
+                # since this is usually a gpu, this speeds up the processing drastically,
+                # since only the reduced results have to be stored.
                 metrics = self._compute(label[i].view(-1), scores[i].view(-1))
                 for key, value in metrics.items():
                     self.buffer.append(key, value.view(1, -1))
 
         return self
 
+    @torch.no_grad()
     def _compute(self, labels: Tensor, scores: Tensor) -> Dict[str, Tensor]:
         """ """
         if len(torch.unique(labels)) != 2:
             raise ValueError("Data must contain IN and OOD samples.")
 
         if labels.shape != scores.shape:
             raise ValueError(f"Inputs have wrong size: {labels.shape} and {scores.shape}")
@@ -184,18 +190,18 @@
 
         p, r, t = binary_precision_recall_curve(-scores, 1 - labels)
         aupr_out = auc(r, p)
 
         fpr = fpr_at_tpr(scores, labels)
 
         return {
-            "AUROC": auroc,
-            "AUPR-IN": aupr_in,
-            "AUPR-OUT": aupr_out,
-            "FPR95TPR": fpr,
+            "AUROC": auroc.cpu(),
+            "AUPR-IN": aupr_in.cpu(),
+            "AUPR-OUT": aupr_out.cpu(),
+            "FPR95TPR": fpr.cpu(),
         }
 
     def compute(self) -> Dict[str, float]:
         """
         Calculate metrics
 
         :return: dictionary with different metrics
```

## Comparing `pytorch_ood-0.1.7.dist-info/LICENSE` & `pytorch_ood-0.1.8.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `pytorch_ood-0.1.7.dist-info/METADATA` & `pytorch_ood-0.1.8.dist-info/METADATA`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: pytorch-ood
-Version: 0.1.7
+Version: 0.1.8
 Summary: A Library for Out-of-Distribution Detection with PyTorch
 Home-page: https://github.com/kkirchheim/pytorch-ood
 Author: Konstantin Kirchheim
 License: Apache 2.0
 Project-URL: Bug Tracker, https://github.com/kkirchheim/pytorch-ood/issues
 Project-URL: repository, https://github.com/kkirchheim/pytorch-ood
 Keywords: OOD,PyTorch,Out-of-Distribution Detection
@@ -18,36 +18,43 @@
 Requires-Dist: torchmetrics (>=1.0.0)
 Requires-Dist: scipy (>=1.7.0)
 Requires-Dist: numpy (>=1.23.0)
 
 PyTorch Out-of-Distribution Detection
 ****************************************
 
-.. image:: https://img.shields.io/badge/docs-online-blue?style=for-the-badge
+|docs| |version| |license| |python-version| |downloads|
+
+
+.. |docs| image:: https://img.shields.io/badge/docs-online-blue?style=for-the-badge
    :target: https://pytorch-ood.readthedocs.io/en/latest/
    :alt: Documentation
-
-.. image:: https://img.shields.io/pypi/v/pytorch-ood?color=light&style=for-the-badge
+.. |version| image:: https://img.shields.io/pypi/v/pytorch-ood?color=light&style=for-the-badge
    :target: https://pypi.org/project/pytorch-ood/
    :alt: License
-
-.. image:: https://img.shields.io/pypi/l/pytorch-ood?style=for-the-badge
+.. |license| image:: https://img.shields.io/pypi/l/pytorch-ood?style=for-the-badge
    :target: https://gitlab.com/kkirchheim/pytorch-ood/-/blob/master/LICENSE
    :alt: License
-
-.. image:: https://img.shields.io/badge/-Python 3.8+-blue?logo=python&logoColor=white&style=for-the-badge
+.. |python-version| image:: https://img.shields.io/badge/-Python 3.8+-blue?logo=python&logoColor=white&style=for-the-badge
    :target: https://www.python.org/
    :alt: Python
-
-.. image:: https://img.shields.io/pypi/dm/pytorch-ood?style=for-the-badge
+.. |downloads| image:: https://img.shields.io/pypi/dm/pytorch-ood?style=for-the-badge
    :target: https://pepy.tech/project/pytorch-ood
    :alt: Downloads
 
 -----
 
+.. image:: docs/_static/pytorch-ood-logo.jpg
+   :align: center
+   :width: 100%
+   :alt: pytorch-ood-logo
+
+-----
+
+
 Out-of-Distribution (OOD) Detection with Deep Neural Networks based on PyTorch.
 
 The library provides:
 
 - Out-of-Distribution Detection Methods
 - Loss Functions
 - Datasets
```

## Comparing `pytorch_ood-0.1.7.dist-info/RECORD` & `pytorch_ood-0.1.8.dist-info/RECORD`

 * *Files 6% similar despite different names*

```diff
@@ -1,84 +1,87 @@
-pytorch_ood/__init__.py,sha256=4XbXVzryu5NMCJoo3hefDGW5poIaqj8F88k_Ixx3zXg,209
+pytorch_ood/__init__.py,sha256=hwHze-0ChwB8Sx_WXfDlmT8dow1KgwXgu-BCX0SGl3o,209
 pytorch_ood/api.py,sha256=eOp1WgYknpzDq8v1hY8V7oWXsI6SQHyCAC1C1dSL6MI,2341
 pytorch_ood/benchmark/__init__.py,sha256=sRY6Sh4aPdcIuuGbvApXq_uAnVBA5xkslpPod3Za-Vo,1404
-pytorch_ood/benchmark/base.py,sha256=2uX-drzDBoz1sG4S2W3Bgsa2riZV_-OsQbP8lf2dD4w,876
+pytorch_ood/benchmark/base.py,sha256=n4YioHXMm7awvqkW8WbpK7v4Zv8CoBcDGXFIR1OT-ps,859
 pytorch_ood/benchmark/img/__init__.py,sha256=Iq_4O1y3fl3cGZyzq5-6bVEf_abHmKwHeuEL-75FhqM,144
-pytorch_ood/benchmark/img/cifar10.py,sha256=rKkIBukethyNUZU3GI7lDOGF8H8hpvcZ9S9SXffu-TI,7122
-pytorch_ood/benchmark/img/cifar100.py,sha256=Pt-C-KNiHTx5LZWiO1O6gY9OTmocbF-ORnLqzfQ18YM,7157
-pytorch_ood/benchmark/img/imagenet.py,sha256=esPDHIg_hNJcssqWDE1FynPlDFQ4px2H-OU24RpBDME,3922
+pytorch_ood/benchmark/img/cifar10.py,sha256=xpkvdO-m-j6y9bnYPiey0vaAjFrWV6PhNa77Tn1_Wsg,7639
+pytorch_ood/benchmark/img/cifar100.py,sha256=U7nIXSi5uhnZVjq2-Q84C-5XE6WxijYvufrW2pvCUBg,7729
+pytorch_ood/benchmark/img/imagenet.py,sha256=6cC_gWJGcO7fkUebQbVPXynMGK33r8sqnzSio-C69Ng,4316
 pytorch_ood/dataset/__init__.py,sha256=4mwJ-YED4MXBlZXeqi04Qg5zxqDMxaszHOupjVy9LAQ,11
 pytorch_ood/dataset/audio/__init__.py,sha256=zYLehvyFdN4DjcGRPpoLVMgSp9HNiAiXAfjxT8CH10k,165
 pytorch_ood/dataset/audio/fsdd.py,sha256=cYKcHtbV7BhGKav_d5qFvNiLRwhvwzWTC93txgu2ezg,3059
-pytorch_ood/dataset/img/__init__.py,sha256=zp_3YUuDlTBJ4VWkW8MZi97gN87iFJCbijA7XSmsTDw,3596
-pytorch_ood/dataset/img/base.py,sha256=J7VLSmD-wHfI6QuGdmhss70bfYQL-rM1HqoCLQBNOh8,2519
+pytorch_ood/dataset/img/__init__.py,sha256=It11vQ60DBS_0CKImKJ6DaQmUfz5FfVYf5j0GzSmtJM,3754
+pytorch_ood/dataset/img/base.py,sha256=QK1jVqsZPL2-KJN71XxEeJlD19QH8P8jCxUdub_NdYQ,2519
 pytorch_ood/dataset/img/chars74k.py,sha256=YA4Hx92fS29jxrpG0zEc09RaRSpmGRdMQMBJ4lyAm2E,4329
 pytorch_ood/dataset/img/cifar.py,sha256=b8a86EKYEAcKb0TCbyODTJ8OVzCtpnDESofMs8EKba8,3097
 pytorch_ood/dataset/img/fooling.py,sha256=10jd8q_hEhpgqrvVCQwDuNmeOEdOHNwRoIZFMpKrIzU,1353
 pytorch_ood/dataset/img/imagenet.py,sha256=MpA-6DD6R1JlIWmS1Ytuik-kfImCnPc-H_Urzj13eqU,5793
 pytorch_ood/dataset/img/mnistc.py,sha256=Wp06GFhRi9_iM6SSe_rUG3GuolMUtuZ_qlR7ctpY4l0,4270
 pytorch_ood/dataset/img/mvtech.py,sha256=Ki5BjbohoWQ-W3tGNAh_1yxidutiujQfBI8tvgEL5R4,5382
-pytorch_ood/dataset/img/ninco.py,sha256=e4r2zu6CexIxwDdfU8gkAlMAFaZn1I2PdWXN3U8s7n8,1919
+pytorch_ood/dataset/img/ninco.py,sha256=4WogDJiew8w-0oi0WLQcctP7Ayslxz-MzloU9y_BnGg,1974
 pytorch_ood/dataset/img/noise.py,sha256=Nt0AF6iFAns0amJ1y3TZHu_WY1qyyb3splVa9Lt-49c,3936
 pytorch_ood/dataset/img/odin.py,sha256=ryBc43DqdRzQ5HMc1X6JNMy_-InR74mbO_-9AAxKOf4,4891
-pytorch_ood/dataset/img/openood.py,sha256=aTPrgaf1mhklOtTUp_qInNxbWi6hzJE1WWest3VtKwA,4715
-pytorch_ood/dataset/img/pixmix.py,sha256=yVwtZ7GH1WPrv7Z3xfDoFyPaMqsjEJWZaiZ-A5fv5rg,8610
+pytorch_ood/dataset/img/openood.py,sha256=hW0L07iYlDHe4YdZbTiKJh3oXBUbO4mT0jTPNWgFE70,4639
+pytorch_ood/dataset/img/pixmix.py,sha256=Rl3bCjKXF48zacmSTdUi0JsLlFuQ6DP8EgvYB-zufGE,8613
 pytorch_ood/dataset/img/streethazards.py,sha256=R4-EMG9voEOshJEBmFtf17KijNcuveuufOVwQVMhqGE,4242
+pytorch_ood/dataset/img/sumnist.py,sha256=fI8k4buyJJBmy5E-wlFHEeW81lfHLxtflQNgbV57giw,4699
 pytorch_ood/dataset/img/textures.py,sha256=VLEv5Md6pNqivOhAp4vUwxuEWfAwfvkPMvnd7zSmDrM,3001
-pytorch_ood/dataset/img/tinyimagenet.py,sha256=OdBRd4mKP819YqmzSl2Ay7zWi2rW2sbxgCc2rWD8f74,3780
+pytorch_ood/dataset/img/tinyimagenet.py,sha256=hG6nesvChj80xUS95P1zpY7di3orFqO-XGXA36ZVv-M,3833
 pytorch_ood/dataset/img/tinyimages.py,sha256=sOw_TIPUxqFaqR5rxAISizkKPNf4BgBcn_kp_bVsiTI,4692
 pytorch_ood/dataset/ossim/__init__.py,sha256=v3qdGAIvb1X6NUHDOK4-jAk1R8nq6Oisjds3ZG2IEp4,507
 pytorch_ood/dataset/ossim/ossim.py,sha256=W6MlSCxrTs_EGlYhB8sSpboh1EDztJJ9YhLVyXv1zSQ,8194
 pytorch_ood/dataset/txt/__init__.py,sha256=wpmf8nqnJkDI-CAgAI-pH07PoV0UKu_miSreNaEX9dw,1049
 pytorch_ood/dataset/txt/multi30k.py,sha256=cIigkuGikYOpKwjgp79nzHvb3WVvtg8Of9TwMlQbp1g,2665
 pytorch_ood/dataset/txt/newsgroups.py,sha256=4tTVikOfXEEUw0CX0vm2pzPKhund5VYNaUYlyGWCNIQ,4002
 pytorch_ood/dataset/txt/reuters.py,sha256=ZWyQ_8VfmPu-9CdDf1PByMqy6HMoxjXGqscmICcHQTk,5954
 pytorch_ood/dataset/txt/stop_words.py,sha256=qSZlcCeyoKKpHfvJJjU6PHcXeG1pia324VdT6jE1HgI,1983
 pytorch_ood/dataset/txt/wiki.py,sha256=Mqvke4I6gh7nvTTw66lcJcamn7TXJ-CnDKSPHYPBHOU,3331
 pytorch_ood/dataset/txt/wmt16.py,sha256=5DVHikenTFOtwn0LaqEEh6xEYeZ9rhQYdrXEzWyDt14,2034
-pytorch_ood/detector/__init__.py,sha256=t9DSwc_zwxEZ6SwlUfJCrpr2R_tlFs6FjaZdns9ptnA,3460
-pytorch_ood/detector/ash.py,sha256=65-ZSpeUGabe4chW0XNeyfIj66fApcbI-x3nLuOUXMI,4750
-pytorch_ood/detector/dice.py,sha256=xArOhczC-dn2fRIpJdP_LEXeeFHaO9SA1i0xzgvEv0M,3050
+pytorch_ood/detector/__init__.py,sha256=ZC2bSxfbPz9geCSGIY0iSBxQNkGwuM7t6yLQM6bAMns,3597
+pytorch_ood/detector/ash.py,sha256=o0Ks8e2zcuqOY8wqwlx14KT5Z8W2bEMV93HKJiQJMxo,4774
+pytorch_ood/detector/dice.py,sha256=G4PeqPyltiNNF0QzCp5e-cLrg7Go4llQ9d3Q82Ejp-c,3075
 pytorch_ood/detector/energy.py,sha256=BE0FhUnWmSauY7JYGEI5hJeIRBV_3JvnW0jNHvIgNZk,2391
 pytorch_ood/detector/entropy.py,sha256=NB6Sbx9zyFgtHzVGVXW-INYfi_0AJFw7lIPuKdkrmac,2085
 pytorch_ood/detector/klmatching.py,sha256=NKv0Boyo8GlpTWPv8O4ESA5cWOL1_OeZILEZYVwJ6sY,4402
-pytorch_ood/detector/knn.py,sha256=Y05wfdlsTnLVCBTt3Idii963il1c7uOWQSvTlNT4Rfw,3207
-pytorch_ood/detector/mahalanobis.py,sha256=3gmzLU84D0BEWZaEGVnWoOdtsU20vrmJAGUOSYcBfIY,7841
+pytorch_ood/detector/knn.py,sha256=Nl5ayI5YJnsyuCMTlxEUoXIFfbWnAVDuHcyeGCKAT9M,3228
+pytorch_ood/detector/mahalanobis.py,sha256=_o3WXX5vCNYlO087B71cw6NVGbQ3cUmvMKsql7REhZI,7823
 pytorch_ood/detector/maxlogit.py,sha256=qlTHoR0ADHDO6M03FTFEYl3uQYFR2O2TkT_MgUSWaTk,1849
-pytorch_ood/detector/mcd.py,sha256=4mDWBtWUFvVfAMNqyeeEveGnf6_kQdnLT46ljmLSCgc,6286
+pytorch_ood/detector/mcd.py,sha256=VppxVj9J-o9JgbzeQKHbV7tk7JBCOT-ufSeZ_HqDyuQ,6383
 pytorch_ood/detector/odin.py,sha256=ubarxZnYeV4sv9fx-Acw0SbobEy6_VmG3SepadVNrr4,5856
-pytorch_ood/detector/react.py,sha256=VCrRDqKvQhkMZQheywTG9Cimh_TwatSjwkjZ995bDfQ,2691
-pytorch_ood/detector/rmd.py,sha256=DV0rUjnrHDhEXQQQ54E7iz3eoyLr6fKTf82T9QGrdoM,4787
-pytorch_ood/detector/she.py,sha256=VwzcYRX9tcJCiDSudgCkB8NtG8mO8C0F9vsxbN3M1Cg,3094
-pytorch_ood/detector/softmax.py,sha256=mAlitqz3OWjKiXlFyokmU06tDip9vQ9ZjiuVw5AZNXg,2660
-pytorch_ood/detector/tscaling.py,sha256=fbHO2y24HqIdcBhGG_KVhfcxvRFFFYQmer8a-e4QIFc,3480
+pytorch_ood/detector/react.py,sha256=QHtToWjzMPTT_Lschdp352fN04TG16oilzalkK21xcU,2709
+pytorch_ood/detector/rmd.py,sha256=U7tg7zdEI180jt6_3SVSnCjDKz_Odbc_lurKAJkUSR0,4812
+pytorch_ood/detector/she.py,sha256=QZsD3rjUWb4OQ1FrvEzCr_GHfMmvQWU067yhYyWaW-Y,3095
+pytorch_ood/detector/softmax.py,sha256=gPkZBQpbxxIZxu3AE5xv1JrFkQun9xB68wtbJi8NRIo,2660
+pytorch_ood/detector/tscaling.py,sha256=BHyzcFeityH1tcg_aLZ3XXM5Lt-PTfvhP9lEY9nATPM,3480
 pytorch_ood/detector/vim.py,sha256=v5Og7bL3oGEXSPfv9WQv1iF0wRofNfKrVttO4EDSE6I,5267
+pytorch_ood/detector/webo.py,sha256=IzNgEgQ_xN9WhBOhI1fpmoX_Jsq-TfBenSBYNS8BP3Q,3436
 pytorch_ood/detector/openmax/__init__.py,sha256=aLSjSeuO9EgxWo5SBbLyOqMLcIMIbgpju9WUL8wtgyE,323
-pytorch_ood/detector/openmax/libnotmr.py,sha256=77aXYFgKVvI7lLAJHCrzJJaDPXKPbcjS5711Q9_uMG0,1594
-pytorch_ood/detector/openmax/numpy.py,sha256=ZVDTyNUSVQ2BOmuqivy8zdlOBWHRGU7RMIvS23_Nu5o,6787
+pytorch_ood/detector/openmax/libnotmr.py,sha256=1FmpPatjFl9e5Vmk6sD_koFVT6WQfkNwTrzsNro1iXI,1636
+pytorch_ood/detector/openmax/numpy.py,sha256=6q8nk_EtN0RV3lsgCSE5FeVTyQk44OOxoeqkv7AGIfM,6789
 pytorch_ood/detector/openmax/torch.py,sha256=I1W9aIbtBamZkA8Rbt9yRREauR3v9MqV2JdMz7HFweE,3317
-pytorch_ood/loss/__init__.py,sha256=p3b1JB96f4YjaDkk1PvaaMFs7tDxUyPW6_lZqR5gFGI,5408
+pytorch_ood/loss/__init__.py,sha256=VtJQYQqhKrYTMa4Y81jU1AVpOeWVJUQ68Ymc49vwoZs,5819
 pytorch_ood/loss/background.py,sha256=d9TNUzeWqsZ7c-EjAGFrWPdOagnQIcJ4hTmTtJD50sc,1589
 pytorch_ood/loss/cac.py,sha256=p1qGxb3i447r6fY1vk3Hh7MWrPUC1KHD4aeh2wUAqmU,3877
 pytorch_ood/loss/center.py,sha256=oKXg_WD3JoQH3Gh38Zy6sTlL0vN44-ksSDCLkbvmnm0,4017
 pytorch_ood/loss/conf.py,sha256=KZdIlKVGqr_rRMR8M9CvDsWtBxTRlG4zgWUVZnBjBkM,2378
 pytorch_ood/loss/crossentropy.py,sha256=JAsScyzOA3GTn2IlHIKU6ya7KyLuu-8r_zhNuLqrNjY,1188
-pytorch_ood/loss/energy.py,sha256=Hns8hACIwuMewFzAu58_uozSRDdkFG-uT8qi-iukPtw,2565
+pytorch_ood/loss/energy.py,sha256=o3qkymDQxOHLnmQ0FseHHa2fZtt2_nfkmZLZDOHqJAg,3394
 pytorch_ood/loss/entropy.py,sha256=K02U4FMyIKZAmOLRnifD_NUPjDZx3zo5LzNdLMT0jpY,2741
 pytorch_ood/loss/ii.py,sha256=ISc-GCRgYnACNMS9GpkfpjJEqPXwChbULp81ykq2M6U,4599
 pytorch_ood/loss/mchad.py,sha256=u-a4tOnDqHeKgwtUb8XPtJqdCsGPtOIUcOnm1p9LgaE,4875
 pytorch_ood/loss/objectosphere.py,sha256=eubnoMhEkmdIhuFAB485CBX-eT0NObxt7FTCkp8HAJE,2667
-pytorch_ood/loss/oe.py,sha256=k9VRWIGyodwk5crz6snu8KGb2VSzGRmP8lCYBxm10WE,2199
-pytorch_ood/loss/svdd.py,sha256=raL0nldFjwk77UM-IDM3B0mGHv50neBZULEPXd5zcaM,5289
+pytorch_ood/loss/oe.py,sha256=y8U34PrZRM1l3a7g6n3APebmSI9U1Se3RlpLM-rxQI8,3009
+pytorch_ood/loss/svdd.py,sha256=e8QTEz8M-2HpRkXO33A_8RaSbiFaV-rW0C9iPPBBoL0,5314
+pytorch_ood/loss/vos.py,sha256=PaKQMv7Kyhcai17aX3bm71l73vHUexXXVn8dX2QY96E,4311
 pytorch_ood/model/__init__.py,sha256=U2iKbCxQMfzkk1q9eeo2Wg2Bm81-UUpN9nA2o5dDwZ0,723
 pytorch_ood/model/centers.py,sha256=4pJ5_fWQ_BOO-QRkwAqc5lGZbUsO6F-ZHOlAmQQWYbs,4684
 pytorch_ood/model/gru.py,sha256=bWgH_9kHofqaqK19K0fm3VA-AimQ6EWuoq3pS6FUo48,1611
 pytorch_ood/model/wrn.py,sha256=TgXij1K1XsX5PEBhBTePn4vDMHY1_smjfhxBV6dumXg,12629
 pytorch_ood/utils/__init__.py,sha256=7HadmLfYRDYnSzWduQZdcIIOzfH-8-_BO8lbq1g-wNk,70
-pytorch_ood/utils/metrics.py,sha256=R5xyrHVQM2-46ZBXCFq9fK0baBGTBr-S0oOgLw1j72k,7226
+pytorch_ood/utils/metrics.py,sha256=MMoxVqoYEBnBQqvP9ztDMnOIFREp4h2u6gXXoQyagRw,7600
 pytorch_ood/utils/transforms.py,sha256=-GyjeFckB2R4gMDDgHkUXmIwX1xbTjljch8wdPItZvY,1953
 pytorch_ood/utils/utils.py,sha256=mod65a7ibuaNcwc7fSe0VdWJFNlqDyCU7whrGMMtY_k,8878
-pytorch_ood-0.1.7.dist-info/LICENSE,sha256=oE9hL_1c39hhaBzmIHPW_5r_4InRrWOI02jxskyrYyQ,11351
-pytorch_ood-0.1.7.dist-info/METADATA,sha256=67JqqzdqOPI9Ss9eQZedinwWB2RCYWW-39gijrtQ5hk,23161
-pytorch_ood-0.1.7.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-pytorch_ood-0.1.7.dist-info/top_level.txt,sha256=DQdb7oLs5bEbGwRkFDMkBe3xeGccj8U-P4eHsi8VOS0,12
-pytorch_ood-0.1.7.dist-info/RECORD,,
+pytorch_ood-0.1.8.dist-info/LICENSE,sha256=oE9hL_1c39hhaBzmIHPW_5r_4InRrWOI02jxskyrYyQ,11351
+pytorch_ood-0.1.8.dist-info/METADATA,sha256=rc90XiRRbx66zI5QEqiL-IIE88BgRcHHtx13Jv0CKmE,23385
+pytorch_ood-0.1.8.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
+pytorch_ood-0.1.8.dist-info/top_level.txt,sha256=DQdb7oLs5bEbGwRkFDMkBe3xeGccj8U-P4eHsi8VOS0,12
+pytorch_ood-0.1.8.dist-info/RECORD,,
```

